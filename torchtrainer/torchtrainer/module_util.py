'''Utility functions and classes for working with Pytorch modules'''

from torch import nn
from functools import partial

class ActivationSampler(nn.Module):
    '''Generates a hook for sampling a layer activation'''

    def __init__(self, model):
        super(ActivationSampler, self).__init__()
        self.model_name = model.__class__.__name__
        self.activation = None
        model.register_forward_hook(self.get_hook())

    def forward(self, x=None):
        return self.activation

    def get_hook(self):
        def hook(model, input, output):
            self.activation = output
        return hook

    def extra_repr(self):
        return f'{self.model_name}'

class Lambda(nn.Module):
    '''Transforms function into a module'''

    def __init__(self, func):
        super().__init__()
        self.func = func

    def forward(self, x): return self.func(x)

class Hooks:
    '''Hooks for storing information about layers.

    The attribute `storage` will contain the layers information. It is a dict
    having layer names as keys and respective values generated by `func`.

    Parameters
    ----------
    module : torch.nn
        ??
    layers : list
        List of torch.nn modules
    func : function
        Function to be registered as hooks. Must have signature func(storage, module, input, output) for
        forward hooks and ?? for backward hooks. `storage` is a dictionary used for storing layer information.
    '''

    def __init__(self, module, layers, func, is_forward=True):

        self.hooks = []

        storage = {}         # For storing information, each layer will be a key here
        layers_dict = {}     # Dict of layer names and actual layers
        # Obtain layer names for hashing. Is there a better way?
        for layer_name, layer in module.named_modules():
            if True in [True for l in layers if layer is l]:
                layers_dict[layer_name] = layer
                storage[layer_name] = {}

        self.layers_dict = layers_dict
        self.storage = storage

        if is_forward:
            self._register_forward_hooks(func)
        else:
            self._register_backward_hooks(func)

    def __del__(self): self.remove_hooks()

    def _register_forward_hooks(self, func):
        '''Register one hook for each layer.'''

        for layer_name, layer in self.layers_dict.items():
            hook_func = self._generate_hook(func, self.storage[layer_name])
            self.hooks.append(layer.register_forward_hook(hook_func))

    def _register_backward_hooks(self):
        pass

    def _generate_hook(self, func, storage):
        '''Generate function to be used in module.register_forward_hook, fixing
        as a first argument to the function an empty dictionary.'''

        return partial(func, storage)

    def to_cpu(self):
        pass

    def remove_hooks(self):
        '''Remove hooks from the network.'''

        for hook in self.hooks:
            hook.remove()

def append_stats(storage, model, input, output, store_act=True, store_weights=False):

    if store_act:
        if 'activation' not in storage:
            storage['activation'] = {}
        if 'mean' not in storage['activation']:
            storage['activation']['mean'] = []
        if 'std' not in storage['activation']:
            storage['activation']['std'] = []

        storage['activation']['mean'].append(output.data.mean().item())
        storage['activation']['std'].append(output.data.std().item()
        #hists.append(output.data.cpu().histc(40,0,10)) #histc isn't implemented on the GPU

    if store_weights:
        if 'weights' not in storage:
            storage['weights'] = {}
        if 'mean' not in storage['weights']:
            storage['weights']['mean'] = []
        if 'std' not in storage['weights']:
            storage['weights']['std'] = []

        storage['weights']['mean'].append(model.data.mean().item())
        storage['weights']['std'].append(model.data.std().item()
        #hists.append(output.data.cpu().histc(40,0,10)) #histc isn't implemented on the GPU