{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes for storing image datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Converted imagedataset.py to torchtrainer/imagedataset.py\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Export cells\n",
    "!python notebook2script.py imagedataset.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset storage\n",
    "#export imagedataset.py\n",
    "'''\n",
    "Dataset storage class\n",
    "'''\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import random\n",
    "from torch.utils.data import dataset as torch_dataset\n",
    "import torch\n",
    "import bisect\n",
    "\n",
    "class ImageDataset(torch_dataset.Dataset):\n",
    "    \"\"\"Dataset storage class.\n",
    "    \n",
    "    Receives an input image and label directory and stores respective images. Images can be\n",
    "    retrieved as follows:\n",
    "    \n",
    "    image_ds = ImageDataset(...)\n",
    "    img, label = image_ds[0]\n",
    "    \n",
    "    or, in case weight_func is not None:\n",
    "    \n",
    "    img, label, weight = image_ds[0]\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_dir : string or pathlib path\n",
    "        Directory containing the images to be read\n",
    "    label_dir : string or pathlib path\n",
    "        Directory containing the labels (segmentations) to be read\n",
    "    name_2_label_map : function\n",
    "        Function with signature name_2_label_map(img_filename) that translates image filenames into\n",
    "        labels filenames. Receives an image filename and returns the filename of an image containing \n",
    "        the respective label\n",
    "    filename_filter : list or function\n",
    "        If list, contains names of the image files that should be kept, other images are ignored\n",
    "        If function, has signature filename_filter(img_filename), receives an image filename and returns\n",
    "        True if the image should be kept. The image is discarded otherwise\n",
    "    img_opener : function\n",
    "        Function with signature img_opener(img_path) for opening the images. Receives an image path \n",
    "        and returns a PIL.Image object. Images should be have uint8 type.\n",
    "    label_opener: function\n",
    "        Function with signature label_opener(label_path) for opening the labels. Receives an label path \n",
    "        and returns a PIL.Image object. The image should contain class indices and have uint8 type\n",
    "    transforms : list of functions\n",
    "        List of functions to be applied for image augmentation. Each function should have the signature\n",
    "        transform(img, label, weight=None) and return a tuple (img, label) in case `weight_func` is None \n",
    "        or (img, label, weight) otherwise. The arguments of the first transform should be PIL.Image objects,\n",
    "        while the return values of the last transform should be float32 torch.Tensor objects.\n",
    "    weight_func : function\n",
    "        Function for generating weights associated to each image. Those can be used for defining masks\n",
    "        or, for instance, weighting the loss function. Must have signature \n",
    "        weight_func(pil_img, pil_label, img_path=None) and return a PIL.Image with F (float32) type\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, img_dir, label_dir, name_2_label_map, filename_filter=None, img_opener=None,\n",
    "                 label_opener=None, transforms=None, weight_func=None):\n",
    "\n",
    "        if isinstance(img_dir, str):\n",
    "            img_dir = Path(img_dir)\n",
    "        if isinstance(label_dir, str):\n",
    "            label_dir = Path(label_dir) \n",
    "        if isinstance(filename_filter, list):\n",
    "            filename_filter = set(filename_filter)\n",
    "        if transforms is None:\n",
    "            transforms = []\n",
    "        if img_opener is None:\n",
    "            img_opener = Image.open\n",
    "        if label_opener is None:\n",
    "            label_opener = Image.open    \n",
    "                   \n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.name_2_label_map = name_2_label_map\n",
    "        self.img_opener = img_opener\n",
    "        self.label_opener = label_opener\n",
    "        self.transforms = transforms\n",
    "        self.weight_func = weight_func\n",
    "\n",
    "        img_file_paths = []\n",
    "        for img_file_path in img_dir.iterdir():\n",
    "            img_filename = img_file_path.name\n",
    "            if filename_filter is None:\n",
    "                img_file_paths.append(img_file_path)\n",
    "            elif isinstance(filename_filter, set):\n",
    "                if img_file_path.stem in filename_filter: img_file_paths.append(img_file_path)       \n",
    "            elif filename_filter(img_filename):\n",
    "                img_file_paths.append(img_file_path)\n",
    "\n",
    "        self.img_file_paths = img_file_paths\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''Returns one item from the dataset. Will return an image and label if weight_func was\n",
    "        not defined during class instantiation or an aditional weight image otherwise.'''\n",
    "        \n",
    "        img_file_path = self.img_file_paths[idx]\n",
    "        \n",
    "        img = self.img_opener(img_file_path)\n",
    "        label_file_path = self.label_path_from_image_path(img_file_path)\n",
    "        label = self.label_opener(label_file_path)\n",
    "        \n",
    "        if self.weight_func is not None:\n",
    "            weight = self.weight_func(img, label, img_file_path)\n",
    "            ret_transf = self.apply_transforms(self.transforms, img, label, weight)\n",
    "        else:\n",
    "            ret_transf = self.apply_transforms(self.transforms, img, label)\n",
    "\n",
    "        return ret_transf\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.img_file_paths)\n",
    "              \n",
    "    def label_path_from_image_path(self, img_file_path):\n",
    "        '''Translates image path to label path.'''\n",
    "        \n",
    "        img_filename = img_file_path.name\n",
    "        return self.label_dir/self.name_2_label_map(img_filename)    \n",
    "    \n",
    "    def check_dataset(self):\n",
    "        '''Check if all images in the dataset can be read, and if the transformations\n",
    "        can be successfully applied. It is usefull to call this function right after\n",
    "        dataset creation.\n",
    "        '''\n",
    "\n",
    "        img_file_paths = self.img_file_paths\n",
    "\n",
    "        shapes = []\n",
    "        for img_idx, img_file_path in enumerate(img_file_paths):\n",
    "            # Check if all data can be obtained\n",
    "            try:\n",
    "                ret_vals = self.__getitem__(img_idx)     # May return multiple items\n",
    "            except Exception:    \n",
    "                raise Exception(f'Cannot get image {img_file_paths[img_idx]} at index {img_idx}\\n')\n",
    "\n",
    "            for idx, ret_val in enumerate(ret_vals):\n",
    "                # Check if data has the same shape\n",
    "                if len(shapes)<(idx+1):\n",
    "                    shapes.append(ret_val.shape)\n",
    "                elif ret_val.shape!=shapes[idx]:\n",
    "                    raise Exception(f\"Data has different shape at index {img_idx}\")            \n",
    "\n",
    "        print('All images read')\n",
    "                \n",
    "    def split_train_val(self, valid_set=0.2):\n",
    "        '''Split dataset into train and validation. Returns two new datasets.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        valid_set : float or list\n",
    "            If float, a fraction `valid_set` of the dataset will be used for validation,\n",
    "            the rest will be used for training.\n",
    "            If list, should containg the names of the files used for validation. The remaining\n",
    "            images will be used for training.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        train_dataset : ImageDataset\n",
    "            Dataset to be used for training\n",
    "        valid_dataset : ImageDataset\n",
    "            Dataset to be used for validation     \n",
    "        '''\n",
    "        \n",
    "        img_file_paths_train, img_file_paths_valid = self.split_train_val_paths(valid_set)\n",
    "        # Hacky way to get parameters passed to __init__ during class construction\n",
    "        init_pars_train = {}\n",
    "        init_code = self.__init__.__code__\n",
    "        for init_par in init_code.co_varnames[1:init_code.co_argcount]:\n",
    "            if (init_par!='filename_filter'):\n",
    "                try:\n",
    "                    init_pars_train[init_par] = self.__getattribute__(init_par)\n",
    "                except AttributeError:\n",
    "                    raise AttributeError('Cannot split dataset, init parameter not registered in class')\n",
    "        init_pars_valid = init_pars_train.copy()\n",
    "        \n",
    "        init_pars_train['filename_filter'] = img_file_paths_train\n",
    "        init_pars_valid['filename_filter'] = img_file_paths_valid\n",
    "        \n",
    "        train_dataset = ImageDataset(**init_pars_train)\n",
    "        valid_dataset = ImageDataset(**init_pars_valid)\n",
    "        \n",
    "        return train_dataset, valid_dataset\n",
    "    \n",
    "    def split_train_val_paths(self, valid_set=0.2):\n",
    "        '''Generates image names to be used for spliting the dataset.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        valid_set : float or list\n",
    "            If float, a fraction `valid_set` of the dataset will be used for validation,\n",
    "            the rest will be used for training.\n",
    "            If list, should containg the names of the files used for validation. The remaining\n",
    "            images will be used for training.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        img_file_paths_train : list\n",
    "            Images used for training\n",
    "        img_file_paths_valid : list\n",
    "            Images used for validation\n",
    "        '''\n",
    "        \n",
    "        img_file_paths = self.img_file_paths\n",
    "        num_images = len(img_file_paths)\n",
    "\n",
    "        img_file_paths_train = []\n",
    "        img_file_paths_valid = []\n",
    "        \n",
    "        if isinstance(valid_set, list):\n",
    "            \n",
    "            valid_set_set = set(valid_set)\n",
    "            for file_idx, img_file_path in enumerate(img_file_paths):\n",
    "                if img_file_path.stem in valid_set_set:\n",
    "                    img_file_paths_valid.append(img_file_path)\n",
    "                else:\n",
    "                    img_file_paths_train.append(img_file_path)\n",
    "\n",
    "            if (len(img_file_paths_train)+len(img_file_paths_valid))!=len(img_file_paths):\n",
    "                print('Warning, some files in validation set not found')\n",
    "\n",
    "        elif isinstance(valid_set, float):\n",
    "            num_images_valid = int(num_images*valid_set)\n",
    "            num_images_train = num_images - num_images_valid\n",
    "\n",
    "            ind_all = list(range(num_images))\n",
    "            random.shuffle(ind_all)\n",
    "            ind_train = ind_all[0:num_images_train]\n",
    "            ind_valid = ind_all[num_images_train:]\n",
    "\n",
    "            img_file_paths_train = [img_file_paths[ind] for ind in ind_train]\n",
    "            img_file_paths_valid = [img_file_paths[ind] for ind in ind_valid]\n",
    "            \n",
    "        img_file_paths_train = [file.stem for file in img_file_paths_train]\n",
    "        img_file_paths_valid = [file.stem for file in img_file_paths_valid]\n",
    "            \n",
    "        return img_file_paths_train, img_file_paths_valid\n",
    "        \n",
    "    def as_tensor(self):\n",
    "        '''Converts all images in the dataset to a single torch tensor.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tensors : torch.Tensor\n",
    "            Tensor with dimensions (num images, num channels, height, width)\n",
    "        '''\n",
    "            \n",
    "        img_file_paths = self.img_file_paths\n",
    "        ret_vals = self.__getitem__(0)           # Open first image to get shape\n",
    "        \n",
    "        num_tensors = len(img_file_paths)\n",
    "        tensors = [torch.zeros((num_tensors, *val.shape), dtype=val.dtype) for val in ret_vals]\n",
    "        \n",
    "        for file_idx, img_file_path in enumerate(img_file_paths):\n",
    "            ret_vals = self.__getitem__(file_idx)\n",
    "            for idx, ret_val in enumerate(ret_vals):\n",
    "                tensors[idx][file_idx] = ret_val\n",
    "\n",
    "        return tensors\n",
    "        \n",
    "    def apply_transforms(self, transforms, img, label, weight=None):\n",
    "        '''Apply transformations stored in transforms\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        transforms : list of functions\n",
    "            List of functions to be applied for image augmentation. Each function should have the signature\n",
    "            transform(img, label, weight=None) and return a tuple (img, label) in case weight is None \n",
    "            or (img, label, weight) otherwise.    \n",
    "        img, label, weight : Image-like\n",
    "            Images to be processed\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        vals : Image-like\n",
    "            Resulting images. Either (img, label) if weight is None or (img, label, weight) otherwise\n",
    "        '''\n",
    "        \n",
    "        if weight is None:\n",
    "            vals = [img, label]\n",
    "        else:\n",
    "            vals = [img, label, weight]\n",
    "        for transform in transforms: \n",
    "                vals = transform(*vals)\n",
    "        return vals\n",
    "    \n",
    "class ImageItem:\n",
    "    \n",
    "    def __init__(self, img, label=None, weight=None):\n",
    "        \n",
    "        self.img = img\n",
    "        self.label = label\n",
    "        self.weight = weight\n",
    "        \n",
    "    def update_img(self, img): self.img = img\n",
    "    def update_label(self, label): self.label = label\n",
    "    def update_weight(self, weight): self.weight = weight\n",
    "    def get_img(self): return self.img\n",
    "    def get_label(self): return self.label\n",
    "    def get_weight(self): return self.weight\n",
    "    def get_items(self): return self.img, self.label, self.weight\n",
    "    def get_defined_items(self): \n",
    "        '''Return items that are not None in a list. If only the image has been defined, return\n",
    "        the image (not a list of one item).'''\n",
    "        \n",
    "        ret = [self.img]\n",
    "        if self.label is not None: ret.append(self.label)\n",
    "        if self.weight is not None: ret.append(self.weight)\n",
    "        if len(ret)==1:\n",
    "            return ret[0]\n",
    "        else:\n",
    "            return ret\n",
    "    \n",
    "    def has_label(self): return self.label is not None\n",
    "    def has_weight(self): return self.weight is not None\n",
    "    \n",
    "    def apply_function(self, func, return_values=False, **kwargs):\n",
    "        \n",
    "        self.img = func(self.img, **kwargs)\n",
    "        if self.label is not None: self.label = func(self.label, **kwargs)\n",
    "        if self.weight is not None: self.weight = func(self.weight, **kwargs)\n",
    "\n",
    "        if return_values:\n",
    "            ret = [self.img]\n",
    "            if self.label is not None: ret.append(self.label)\n",
    "            if self.weight is not None: ret.append(self.weight)\n",
    "            return ret\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "code_folding": [
     192,
     199,
     209,
     220,
     248,
     255
    ]
   },
   "outputs": [],
   "source": [
    "# Patchwise image dataset storage\n",
    "#export imagedataset.py\n",
    "'''\n",
    "Patchwise image dataset storage\n",
    "'''\n",
    "\n",
    "class ImagePatchDataset(ImageDataset):\n",
    "    \n",
    "    def __init__(self, patch_size, *args, stride=None, patch_transforms=None, **kwargs):\n",
    "    \n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        if stride is None:\n",
    "            stride = patch_size\n",
    "        if isinstance(stride, int):\n",
    "            stride = (stride,)*len(patch_size)\n",
    "        if patch_transforms is None:\n",
    "            patch_transforms = []\n",
    "            \n",
    "        if len(patch_size)!=len(stride):\n",
    "            raise ValueError('`patch_size` and `stride` must have same length')\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.stride = stride\n",
    "        self.patch_transforms = patch_transforms\n",
    "        \n",
    "        self.generate_patches_corners_for_dataset()\n",
    "   \n",
    "    def __getitem__(self, idx):\n",
    "        '''Returns one item from the dataset. Will return an image and label if weight_func was\n",
    "        not defined during class instantiation or an aditional weight image otherwise.'''\n",
    "        \n",
    "        img_index, patch_corners = self.patches_corners[idx]\n",
    "        ret = super().__getitem__(img_index)\n",
    "        if self.weight_func is None:\n",
    "            img, label = ret\n",
    "        else:\n",
    "            img, label, weight = ret\n",
    "        \n",
    "        #first_row, first_col, last_row, last_col = patch_corners\n",
    "        img_patch = self.crop_img(img, patch_corners)\n",
    "        label_patch = self.crop_img(label, patch_corners)\n",
    "            \n",
    "        if self.weight_func is not None:\n",
    "            weight_patch = self.crop_img(weight, patch_corners)\n",
    "            # Apply transforms on patch\n",
    "            ret_transf = self.apply_transforms(self.patch_transforms, img_patch, label_patch, weight_patch)\n",
    "        else:\n",
    "            ret_transf = self.apply_transforms(self.patch_transforms, img_patch, label_patch)        \n",
    "              \n",
    "        return ret_transf\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.patches_corners)\n",
    "       \n",
    "    def generate_patches_corners_for_dataset(self, img_shape=None):\n",
    "        '''If img_shape is None, generates indices by opening each image to get the\n",
    "        respective shape. This is useful when images have distinct sizes. If img_shape\n",
    "        is not None, uses that shape and the images are not opened, which is much faster.'''\n",
    "     \n",
    "        if img_shape is None:\n",
    "            must_open = True\n",
    "        else:\n",
    "            must_open = False\n",
    "\n",
    "        self.patches_corners = []\n",
    "        self.patch_index_accumulator = [0]\n",
    "        for img_idx, img_file_path in enumerate(self.img_file_paths):\n",
    "            if must_open:\n",
    "                try:\n",
    "                    ret = super().__getitem__(img_idx)\n",
    "                except Exception:    \n",
    "                    raise Exception(f'Cannot get image {img_file_path}\\n')\n",
    "                img_shape = self.get_shape(ret[0])\n",
    "                if len(self.patch_size)!=len(img_shape):\n",
    "                    raise ValueError('Length of `patch_size` must be the same as image dimension')\n",
    "            patches_corners_img = self.generate_patches_corners_for_image(self.patch_size, self.stride, img_shape)\n",
    "            self.patches_corners.extend(zip([img_idx]*len(patches_corners_img), patches_corners_img))\n",
    "            \n",
    "    def generate_patches_corners_for_image(self, patch_size, stride, img_shape):\n",
    "            \n",
    "        if len(img_shape)==2:\n",
    "            patches_corners = self.generate_patches_corners_for_2d_image(patch_size, stride, img_shape)\n",
    "        elif len(img_shape)==3:\n",
    "            patches_corners = self.generate_patches_corners_for_3d_image(patch_size, stride, img_shape)\n",
    "        else:\n",
    "            raise Exception('Image must be 2D or 3D')\n",
    "            \n",
    "        return patches_corners\n",
    "    \n",
    "    def generate_patches_corners_for_2d_image(self, patch_size, stride, img_shape):\n",
    "            \n",
    "        #patch_count = 0\n",
    "        patches_corners = []\n",
    "        for row in range(0, img_shape[0]-patch_size[0]+stride[0], stride[0]):\n",
    "            if (row+patch_size[0])>=img_shape[0]:\n",
    "                # Do not go over image border\n",
    "                row = img_shape[0] - patch_size[0]\n",
    "            for col in range(0, img_shape[1]-patch_size[1]+stride[1], stride[1]):\n",
    "                if (col+patch_size[1])>=img_shape[1]:\n",
    "                    # Do not go over image border\n",
    "                    col = img_shape[1] - patch_size[1]\n",
    "                    \n",
    "                patch_corners = (slice(row, row+patch_size[0]), slice(col, col+patch_size[1]))\n",
    "                patches_corners.append(patch_corners)\n",
    "                #patch_count += 1\n",
    "        #self.patch_index_accumulator.append(self.patch_index_accumulator[-1] + patch_count)\n",
    "        return patches_corners\n",
    "\n",
    "    def generate_patches_corners_for_3d_image(self, patch_size, stride, img_shape):\n",
    "            \n",
    "        #patch_count = 0\n",
    "        patches_corners = []\n",
    "        for plane in range(0, img_shape[0]-patch_size[0]+stride[0], stride[0]):\n",
    "            if (plane+patch_size[0])>=img_shape[0]:\n",
    "                # Do not go over image border\n",
    "                plane = img_shape[0] - patch_size[0]\n",
    "            for row in range(0, img_shape[1]-patch_size[1]+stride[1], stride[1]):\n",
    "                if (row+patch_size[1])>=img_shape[1]:\n",
    "                    # Do not go over image border\n",
    "                    row = img_shape[1] - patch_size[1]\n",
    "                for col in range(0, img_shape[2]-patch_size[2]+stride[2], stride[2]):\n",
    "                    if (col+patch_size[2])>=img_shape[2]:\n",
    "                        # Do not go over image border\n",
    "                        col = img_shape[2] - patch_size[2]\n",
    "\n",
    "                    patch_corners = (slice(plane, plane+patch_size[0]), slice(row, row+patch_size[1]),\n",
    "                                     slice(col, col+patch_size[2]))\n",
    "                    patches_corners.append(patch_corners)\n",
    "\n",
    "        return patches_corners\n",
    "       \n",
    "    def get_patch_from_index(self, index, img_shape):\n",
    "        pass\n",
    "              \n",
    "    def get_shape(self, img, warn=True):\n",
    "        \n",
    "        if isinstance(img, Image.Image):\n",
    "            img_shape = (img.height, img.width)\n",
    "        elif isinstance(img, torch.Tensor):\n",
    "            img_shape = img.shape     \n",
    "            if (img.ndim==3): \n",
    "                if img_shape[-3]<=3:\n",
    "                    # Consider that third to last dimension is for color\n",
    "                    img_shape = img_shape[-2:]\n",
    "                else:\n",
    "                    img_shape = img_shape[-3:]\n",
    "            if (img.ndim==4): \n",
    "                img_shape = img_shape[-3:]\n",
    "        elif isinstance(img, np.ndarray):\n",
    "            img_shape = img.shape    \n",
    "            if img.ndim==3: \n",
    "                if img_shape[-1]<=3:\n",
    "                    # Consider that last dimension is for color\n",
    "                    img_shape = img_shape[-3:-1]\n",
    "                else:\n",
    "                    img_shape = img_shape[-3:]\n",
    "            elif img.ndim==4:\n",
    "                img_shape = img_shape[-3:]\n",
    "        else:\n",
    "            raise AttributeError(\"Image is not a PIL, Tensor or ndarray. Cannot safely infer shape\")\n",
    "            \n",
    "        if min(img_shape)<=3:\n",
    "            print(f'Warning, inferred shape {img_shape} is probably incorrect. Sizes smaller than 4 are being discarded')\n",
    "            img_shape = filter(lambda v:v>3, img_shape)\n",
    "    \n",
    "        return img_shape\n",
    "    \n",
    "    def crop_img(self, img, patch_corners):\n",
    "        '''TODO: Decide if we should include imgaug crop. Otherwise crop will not work if last\n",
    "        transform of self.img_transforms is from imgaug.'''\n",
    "        \n",
    "        if isinstance(img, Image.Image):\n",
    "            first_row, last_row = patch_corners[0].start, patch_corners[0].stop\n",
    "            first_col, last_col = patch_corners[1].start, patch_corners[1].stop\n",
    "            img_patch = img.crop([first_col, first_row, last_col, last_row])\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            # Crop from trailing dimensions\n",
    "            img_patch = img[(...,)+patch_corners]\n",
    "        else:\n",
    "            try:\n",
    "                img_patch = img[patch_corners]\n",
    "            except Exception:\n",
    "                raise IndexError(f'Cannot crop image of type {type(img)}')\n",
    "                \n",
    "        #import pdb; pdb.set_trace()\n",
    "        \n",
    "        return img_patch\n",
    "    \n",
    "    @classmethod\n",
    "    def get_image_from_patches(cls, patches, stride, img_shape, operation='max'):\n",
    "        \n",
    "        if not isinstance(patches[0], torch.Tensor):\n",
    "            patches = map(torch.tensor, patches)\n",
    "            \n",
    "        patch_size = patches[0].shape\n",
    "        img = torch.zeros(img_shape, dtype=patches[0].dtype)\n",
    "        patches_corners = cls.generate_patches_corners_for_image(patch_size, stride, img_shape)\n",
    "        \n",
    "        if operation=='mean':\n",
    "            img_count = torch.zeros(img_shape, dtype=int)\n",
    "        for patch_corners, patch in zip(patches_corners, patches):\n",
    "            img_patch = img[patch_corners]\n",
    "            if operation=='max':\n",
    "                img_patch[:] = torch.where(img_patch>patch, img_patch, patch)\n",
    "            elif operation=='min':\n",
    "                img_patch[:] = torch.where(img_patch<patch, img_patch, patch)\n",
    "            elif operation=='mean':\n",
    "                img_patch[:] = img_patch + patch\n",
    "                img_count[patch_corners] += 1\n",
    "                \n",
    "        if operation=='mean':\n",
    "            mask = img_count>0\n",
    "            img[mask] = img[mask]/img_count[mask]\n",
    "            \n",
    "        return img\n",
    "                \n",
    "    # Functions with some ideas, not necessary for class\n",
    "    def crop_2d_img(self, img, patch_corners):\n",
    "        \n",
    "        first_row, first_col, last_row, last_col = patch_corners\n",
    "        if isinstance(img, Image.Image):\n",
    "            img_patch = img.crop([first_col, first_row, last_col, last_row])\n",
    "        else:\n",
    "            img_patch = img[first_row:last_row, first_col:last_col]\n",
    "\n",
    "        return img_patch\n",
    "    \n",
    "    def crop_3d_img(self, img, patch_corners):\n",
    "        \n",
    "        first_plane, first_row, first_col, last_plane, last_row, last_col = patch_corners\n",
    "        \n",
    "        if isinstance(img, Image.Image):\n",
    "            raise ValueError('Cannot interpret PIL image as 3D')\n",
    "        else:\n",
    "            img_patch = img[first_plane:last_plane, first_row:last_row, first_col:last_col]\n",
    "            \n",
    "        return img_patch\n",
    "                                 \n",
    "    def generate_patch_index_accumulator(self, patch_size, stride=None, img_shape=None):\n",
    "        '''If img_shape is None, generates indices by opening each image to get the\n",
    "        respective shape. This is useful when images have distinct sizes. If img_shape\n",
    "        is not None, uses that shape and the images are not opened, which is much faster.'''\n",
    "     \n",
    "        if img_shape is None:\n",
    "            must_open = True\n",
    "        else:\n",
    "            must_open = False\n",
    "\n",
    "        index_accumulator = [0]\n",
    "        img_shapes = []\n",
    "        for img_file_path in self.img_file_paths:\n",
    "            if must_open:\n",
    "                try:\n",
    "                    img = self.img_opener(img_file_path)\n",
    "                except Exception:    \n",
    "                    raise Exception(f'Cannot open image {img_file_path}\\n')\n",
    "                \n",
    "            img_shape = self.get_shape(img)\n",
    "            num_patches = self._num_patches_in_img(patch_size, stride, img_shape)\n",
    "            \n",
    "            img_shapes.append(img_shape)\n",
    "            index_accumulator.append(index_accumulator[-1] + num_patches)\n",
    "        \n",
    "        self.img_shapes = img_shapes\n",
    "        self.patch_index_accumulator = index_accumulator\n",
    "                \n",
    "    def get_patch_from_global_index(self, index):\n",
    "        \n",
    "        img_index = bisect.bisect(self.patch_index_accumulator, index) - 1\n",
    "        img_shape = self.img_shapes[img_index]\n",
    "        patch_index = index - self.patch_index_accumulator[img_index]\n",
    "        get_patch_from_index(self, patch_index, img_shape)      \n",
    "        \n",
    "    def _num_patches_in_img(self, patch_size, stride, img_shape):\n",
    "        \n",
    "        num_p_rows = (img_shape[0]-patch_size[0])//stride[0] + 1\n",
    "        if num_p_rows*stride[0]!=img_shape[0]:\n",
    "            # If patches do not fit perfectly\n",
    "            num_p_rows += 1\n",
    "        num_p_cols = (img_shape[1]-patch_size[1])//stride[1] + 1\n",
    "        if num_p_cols*stride[1]!=img_shape[1]:\n",
    "            # If patches do not fit perfectly\n",
    "            num_p_cols += 1\n",
    "            \n",
    "        return num_p_rows*num_p_cols\n",
    "                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "code_folding": [
     0,
     10,
     30
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All images read\n"
     ]
    }
   ],
   "source": [
    "# Create dataloaders\n",
    "\n",
    "from pathlib import Path\n",
    "from torchtrainer import img_util\n",
    "from torchtrainer import transforms\n",
    "import imgaug.augmenters as iaa\n",
    "import re\n",
    "from torch.utils.data import dataloader as torch_dataloader\n",
    "from functools import partial\n",
    "\n",
    "def name_2_label_map(img_filename):\n",
    "    '''Maps image names to labels names in the DRIVE dataset\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    img_filename : string\n",
    "        Filename of an image\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    label_filename : string\n",
    "        Filename of the corresponding label image\n",
    "    '''  \n",
    "    \n",
    "    m = re.match('(\\d\\d)_[training|test]', img_filename)\n",
    "    index = m.group(1)\n",
    "    label_filename = index + '_manual1.gif'\n",
    "    \n",
    "    return label_filename\n",
    "\n",
    "def filename_filter(img_filename, test=False):\n",
    "    '''Filter DRIVE images for getting only training or test images.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    img_filename : string\n",
    "        Filename of an image\n",
    "    test : bool\n",
    "        If True, the function checks if the image should be in the test set\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        If `test` is False, returns True if the image should be in the training set and\n",
    "        False if it should be in the test set. Return values are reversed if `test` is True\n",
    "    '''\n",
    "    \n",
    "    if test:\n",
    "        return 'test' in img_filename\n",
    "    else:\n",
    "        return 'test' not in img_filename\n",
    "\n",
    "root_dir = Path('../drive/data/DRIVE')\n",
    "img_dir = root_dir/'images'\n",
    "label_dir = root_dir/'labels'\n",
    "mask_dir = root_dir/'mask'\n",
    "\n",
    "# Create functions for using in the dataset creation\n",
    "img_opener_partial = partial(img_util.pil_img_opener, channel=None)\n",
    "label_opener_partial = partial(img_util.pil_img_opener, is_label=True)\n",
    "\n",
    "# Image transformations\n",
    "imgaug_seq = iaa.Sequential([\n",
    "                            iaa.Resize({\"height\": 560, \"width\": 560}),\n",
    "                            ])\n",
    "transform_funcs = transforms.seq_pil_to_imgaug_to_tensor(imgaug_seq)\n",
    "\n",
    "# Create ImageDataset instance\n",
    "dataset = ImageDataset(img_dir, label_dir, name_2_label_map=name_2_label_map, filename_filter=filename_filter, \n",
    "                       img_opener=img_opener_partial, label_opener=label_opener_partial, transforms=transform_funcs)\n",
    "\n",
    "dataset.check_dataset()\n",
    "\n",
    "train_ds, valid_ds = dataset.split_train_val(0.2)\n",
    "\n",
    "# We can grab all images as tensor\n",
    "x_train,y_train = train_ds.as_tensor()\n",
    "x_valid,y_validalid = valid_ds.as_tensor()\n",
    "\n",
    "# Or create a dataloader\n",
    "train_dl = torch_dataloader.DataLoader(train_ds, batch_size=4, shuffle=True)\n",
    "valid_dl = torch_dataloader.DataLoader(valid_ds, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def img_opener(img_file_path, is_label=False):\n",
    "    \n",
    "    if is_label: \n",
    "        img = torch.randint(0, 2, (50, 500, 500)) #np.random.randint(0, 2, (50, 500, 500))\n",
    "    else:\n",
    "        img = torch.randint(0, 256, (50, 500, 500)) #np.random.randint(0, 256, (50, 500, 500))\n",
    "        \n",
    "    return img\n",
    "\n",
    "patch_size = (25, 100, 100)\n",
    "stride = (25, 50, 50)\n",
    "\n",
    "\n",
    "imgaug_seq = iaa.Sequential([\n",
    "                            iaa.Resize({\"height\": 500, \"width\": 500}),\n",
    "                            ])\n",
    "img_transform_funcs = [] #transforms.seq_pil_to_imgaug_to_tensor(imgaug_seq)\n",
    "\n",
    "# Patch transformations\n",
    "imgaug_seq = iaa.Sequential([\n",
    "                            iaa.Resize({\"height\": 120, \"width\": 120}),\n",
    "                            ])\n",
    "patch_transform_funcs = [partial(transforms.transf_to_numpy, is_3d=False), transforms.transf_normalize] \n",
    "                           #[transforms.transf_to_imgaug, transforms.translate_imagaug_seq(imgaug_seq), \n",
    "                           #transforms.transf_to_tensor]\n",
    "\n",
    "\n",
    "\n",
    "# Create ImageDataset instance\n",
    "dataset = ImagePatchDataset(patch_size, img_dir, label_dir, name_2_label_map=name_2_label_map, filename_filter=filename_filter,\n",
    "                            img_opener=img_opener, label_opener=partial(img_opener, True), \n",
    "                            transforms=img_transform_funcs, patch_transforms=patch_transform_funcs, stride=stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100, 25)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[45][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67.93997008967241"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.power(560*560, 1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = iaa.Resize({\"height\": 120, \"width\": 120})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Got an unknown datatype for argument 'segmentation_maps'. Expected datatypes were: None, array[int], array[uint], array[bool], SegmentationMapsOnImage, iterable[empty], iterable-array[int], iterable-array[uint], iterable-array[bool], iterable-SegmentationMapsOnImage. Got: str.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-ce981ddb714d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msegmentation_maps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\Miniconda3\\lib\\site-packages\\imgaug\\augmenters\\meta.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m         \u001b[1;34m\"\"\"Alias for :func:`~imgaug.augmenters.meta.Augmenter.augment`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maugment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprocesses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxtasksperchild\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Miniconda3\\lib\\site-packages\\imgaug\\augmenters\\meta.py\u001b[0m in \u001b[0;36maugment\u001b[1;34m(self, return_batch, hooks, **kwargs)\u001b[0m\n\u001b[0;32m   1977\u001b[0m         )\n\u001b[0;32m   1978\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1979\u001b[1;33m         \u001b[0mbatch_aug\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maugment_batch_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1980\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1981\u001b[0m         \u001b[1;31m# return either batch or tuple of augmentables, depending on what\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Miniconda3\\lib\\site-packages\\imgaug\\augmenters\\meta.py\u001b[0m in \u001b[0;36maugment_batch_\u001b[1;34m(self, batch, parents, hooks)\u001b[0m\n\u001b[0;32m    594\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUnnormalizedBatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m             \u001b[0mbatch_unnorm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m             \u001b[0mbatch_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_normalized_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    597\u001b[0m             \u001b[0mbatch_inaug\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_norm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_batch_in_augmentation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Miniconda3\\lib\\site-packages\\imgaug\\augmentables\\batches.py\u001b[0m in \u001b[0;36mto_normalized_batch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m                 self.heatmaps_unaug, shapes),\n\u001b[0;32m    205\u001b[0m             segmentation_maps=nlib.normalize_segmentation_maps(\n\u001b[1;32m--> 206\u001b[1;33m                 self.segmentation_maps_unaug, shapes),\n\u001b[0m\u001b[0;32m    207\u001b[0m             keypoints=nlib.normalize_keypoints(\n\u001b[0;32m    208\u001b[0m                 self.keypoints_unaug, shapes),\n",
      "\u001b[1;32mE:\\Miniconda3\\lib\\site-packages\\imgaug\\augmentables\\normalization.py\u001b[0m in \u001b[0;36mnormalize_segmentation_maps\u001b[1;34m(inputs, shapes)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_preprocess_shapes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m     \u001b[0mntype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimate_segmaps_norm_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m     _assert_exactly_n_shapes_partial = functools.partial(\n\u001b[0;32m    197\u001b[0m         \u001b[0m_assert_exactly_n_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Miniconda3\\lib\\site-packages\\imgaug\\augmentables\\normalization.py\u001b[0m in \u001b[0;36mestimate_segmaps_norm_type\u001b[1;34m(segmentation_maps)\u001b[0m\n\u001b[0;32m   1085\u001b[0m     ]\n\u001b[0;32m   1086\u001b[0m     _assert_is_of_norm_type(\n\u001b[1;32m-> 1087\u001b[1;33m         type_str, valid_type_strs, \"segmentation_maps\")\n\u001b[0m\u001b[0;32m   1088\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtype_str\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1089\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Miniconda3\\lib\\site-packages\\imgaug\\augmentables\\normalization.py\u001b[0m in \u001b[0;36m_assert_is_of_norm_type\u001b[1;34m(type_str, valid_type_strs, arg_name)\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[1;34m\"Got an unknown datatype for argument '%s'. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1054\u001b[0m         \"Expected datatypes were: %s. Got: %s.\" % (\n\u001b[1;32m-> 1055\u001b[1;33m             arg_name, \", \".join(valid_type_strs), type_str))\n\u001b[0m\u001b[0;32m   1056\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Got an unknown datatype for argument 'segmentation_maps'. Expected datatypes were: None, array[int], array[uint], array[bool], SegmentationMapsOnImage, iterable[empty], iterable-array[int], iterable-array[uint], iterable-array[bool], iterable-SegmentationMapsOnImage. Got: str."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "img = np.zeros([30,30])\n",
    "res = r(image=img, segmentation_maps='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]), None)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
