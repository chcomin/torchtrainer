{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes for storing image datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python: can't open file 'notebook2script.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Export cells\n",
    "!python notebook2script.py imagedataset.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Dataset storage\n",
    "#export imagedataset.py\n",
    "'''\n",
    "Dataset storage class\n",
    "'''\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import random\n",
    "from torch.utils.data import dataset as torch_dataset\n",
    "import torch\n",
    "import bisect\n",
    "import copy\n",
    "\n",
    "class ImageDataset(torch_dataset.Dataset):\n",
    "    \"\"\"Dataset storage class.\n",
    "    \n",
    "    Receives an input image and label directory and stores respective images. Images can be\n",
    "    retrieved as follows:\n",
    "    \n",
    "    image_ds = ImageDataset(...)\n",
    "    img, label = image_ds[0]\n",
    "    \n",
    "    or, in case weight_func is not None:\n",
    "    \n",
    "    img, label, weight = image_ds[0]\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_dir : string or pathlib path\n",
    "        Directory containing the images to be read\n",
    "    label_dir : string or pathlib path\n",
    "        Directory containing the labels (segmentations) to be read\n",
    "    name_2_label_map : function\n",
    "        Function with signature name_2_label_map(img_filename) that translates image filenames into\n",
    "        labels filenames. Receives an image filename and returns the filename of an image containing \n",
    "        the respective label\n",
    "    filename_filter : list or function\n",
    "        If list, contains names of the image files that should be kept, other images are ignored\n",
    "        If function, has signature filename_filter(img_filename), receives an image filename and returns\n",
    "        True if the image should be kept. The image is discarded otherwise\n",
    "    img_opener : function\n",
    "        Function with signature img_opener(img_path) for opening the images. Receives an image path \n",
    "        and returns a PIL.Image object. Images should be have uint8 type.\n",
    "    label_opener: function\n",
    "        Function with signature label_opener(label_path) for opening the labels. Receives an label path \n",
    "        and returns a PIL.Image object. The image should contain class indices and have uint8 type\n",
    "    transforms : list of functions\n",
    "        List of functions to be applied for image augmentation. Each function should have the signature\n",
    "        transform(img, label, weight=None) and return a tuple (img, label) in case `weight_func` is None \n",
    "        or (img, label, weight) otherwise. The arguments of the first transform should be PIL.Image objects,\n",
    "        while the return values of the last transform should be float32 torch.Tensor objects.\n",
    "    weight_func : function\n",
    "        Function for generating weights associated to each image. Those can be used for defining masks\n",
    "        or, for instance, weighting the loss function. Must have signature \n",
    "        weight_func(pil_img, pil_label, img_path=None) and return a PIL.Image with F (float32) type\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, img_dir, label_dir, name_2_label_map, filename_filter=None, img_opener=None,\n",
    "                 label_opener=None, transforms=None, weight_func=None):\n",
    "\n",
    "        if isinstance(img_dir, str):\n",
    "            img_dir = Path(img_dir)\n",
    "        if isinstance(label_dir, str):\n",
    "            label_dir = Path(label_dir) \n",
    "        if isinstance(filename_filter, list):\n",
    "            filename_filter = set(filename_filter)\n",
    "        if transforms is None:\n",
    "            transforms = []\n",
    "        if img_opener is None:\n",
    "            img_opener = Image.open\n",
    "        if label_opener is None:\n",
    "            label_opener = Image.open    \n",
    "                   \n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.name_2_label_map = name_2_label_map\n",
    "        self.img_opener = img_opener\n",
    "        self.label_opener = label_opener\n",
    "        self.transforms = transforms\n",
    "        self.weight_func = weight_func\n",
    "\n",
    "        img_file_paths = []\n",
    "        for img_file_path in img_dir.iterdir():\n",
    "            img_filename = img_file_path.name\n",
    "            if filename_filter is None:\n",
    "                img_file_paths.append(img_file_path)\n",
    "            elif isinstance(filename_filter, set):\n",
    "                if img_file_path.stem in filename_filter: img_file_paths.append(img_file_path)       \n",
    "            elif filename_filter(img_filename):\n",
    "                img_file_paths.append(img_file_path)\n",
    "\n",
    "        self.img_file_paths = img_file_paths\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''Returns one item from the dataset. Will return an image and label if weight_func was\n",
    "        not defined during class instantiation or an aditional weight image otherwise.'''\n",
    "        \n",
    "        img_file_path = self.img_file_paths[idx]\n",
    "        \n",
    "        img = self.img_opener(img_file_path)\n",
    "        label_file_path = self.label_path_from_image_path(img_file_path)\n",
    "        label = self.label_opener(label_file_path)\n",
    "        \n",
    "        if self.weight_func is not None:\n",
    "            weight = self.weight_func(img, label, img_file_path)\n",
    "            ret_transf = self.apply_transforms(self.transforms, img, label, weight)\n",
    "        else:\n",
    "            ret_transf = self.apply_transforms(self.transforms, img, label)\n",
    "\n",
    "        return ret_transf\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.img_file_paths)\n",
    "    \n",
    "    def subset(self, filename_filter):\n",
    "        \n",
    "        img_dir = copy.copy(self.img_dir)\n",
    "        label_dir = copy.copy(self.label_dir)\n",
    "        name_2_label_map = self.name_2_label_map\n",
    "        img_opener = self.img_opener\n",
    "        label_opener = self.label_opener\n",
    "        transforms = copy.copy(self.transforms)\n",
    "        weight_func = self.weight_func\n",
    "        return self.__class__(img_dir, label_dir, name_2_label_map, filename_filter=filename_filter, img_opener=img_opener,\n",
    "                              label_opener=label_opener, transforms=transforms, weight_func=weight_func)\n",
    "             \n",
    "    def label_path_from_image_path(self, img_file_path):\n",
    "        '''Translates image path to label path.'''\n",
    "        \n",
    "        img_filename = img_file_path.name\n",
    "        return self.label_dir/self.name_2_label_map(img_filename)    \n",
    "    \n",
    "    def check_dataset(self):\n",
    "        '''Check if all images in the dataset can be read, and if the transformations\n",
    "        can be successfully applied. It is usefull to call this function right after\n",
    "        dataset creation.\n",
    "        '''\n",
    "\n",
    "        img_file_paths = self.img_file_paths\n",
    "\n",
    "        shapes = []\n",
    "        for img_idx, img_file_path in enumerate(img_file_paths):\n",
    "            # Check if all data can be obtained\n",
    "            try:\n",
    "                ret_vals = self.__getitem__(img_idx)     # May return multiple items\n",
    "            except Exception:    \n",
    "                raise Exception(f'Cannot get image {img_file_paths[img_idx]} at index {img_idx}\\n')\n",
    "\n",
    "            for idx, ret_val in enumerate(ret_vals):\n",
    "                # Check if data has the same shape\n",
    "                if len(shapes)<(idx+1):\n",
    "                    shapes.append(ret_val.shape)\n",
    "                elif ret_val.shape!=shapes[idx]:\n",
    "                    raise Exception(f\"Data has different shape at index {img_idx}\")            \n",
    "\n",
    "        print('All images read')\n",
    "                \n",
    "    def split_train_val(self, valid_set=0.2):\n",
    "        '''Split dataset into train and validation. Returns two new datasets.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        valid_set : float or list\n",
    "            If float, a fraction `valid_set` of the dataset will be used for validation,\n",
    "            the rest will be used for training.\n",
    "            If list, should containg the names of the files used for validation. The remaining\n",
    "            images will be used for training.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        train_dataset : ImageDataset\n",
    "            Dataset to be used for training\n",
    "        valid_dataset : ImageDataset\n",
    "            Dataset to be used for validation     \n",
    "        '''\n",
    "        \n",
    "        img_file_paths_train, img_file_paths_valid = self.split_train_val_paths(valid_set)\n",
    "        '''# Hacky way to get parameters passed to __init__ during class construction\n",
    "        init_pars_train = {}\n",
    "        init_code = self.__init__.__code__\n",
    "        for init_par in init_code.co_varnames[1:init_code.co_argcount]:\n",
    "            if (init_par!='filename_filter'):\n",
    "                try:\n",
    "                    init_pars_train[init_par] = self.__getattribute__(init_par)\n",
    "                except AttributeError:\n",
    "                    raise AttributeError('Cannot split dataset, init parameter not registered in class')\n",
    "        init_pars_valid = init_pars_train.copy()\n",
    "        \n",
    "        init_pars_train['filename_filter'] = img_file_paths_train\n",
    "        init_pars_valid['filename_filter'] = img_file_paths_valid'''\n",
    "        \n",
    "        train_dataset = self.subset(img_file_paths_train)\n",
    "        valid_dataset = self.subset(img_file_paths_valid)\n",
    "        \n",
    "        return train_dataset, valid_dataset\n",
    "    \n",
    "    def split_train_val_paths(self, valid_set=0.2):\n",
    "        '''Generates image names to be used for spliting the dataset.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        valid_set : float or list\n",
    "            If float, a fraction `valid_set` of the dataset will be used for validation,\n",
    "            the rest will be used for training.\n",
    "            If list, should containg the names of the files used for validation. The remaining\n",
    "            images will be used for training.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        img_file_paths_train : list\n",
    "            Images used for training\n",
    "        img_file_paths_valid : list\n",
    "            Images used for validation\n",
    "        '''\n",
    "        \n",
    "        img_file_paths = self.img_file_paths\n",
    "        num_images = len(img_file_paths)\n",
    "\n",
    "        img_file_paths_train = []\n",
    "        img_file_paths_valid = []\n",
    "        \n",
    "        if isinstance(valid_set, list):\n",
    "            \n",
    "            valid_set_set = set(valid_set)\n",
    "            for file_idx, img_file_path in enumerate(img_file_paths):\n",
    "                if img_file_path.stem in valid_set_set:\n",
    "                    img_file_paths_valid.append(img_file_path)\n",
    "                else:\n",
    "                    img_file_paths_train.append(img_file_path)\n",
    "\n",
    "            if (len(img_file_paths_train)+len(img_file_paths_valid))!=len(img_file_paths):\n",
    "                print('Warning, some files in validation set not found')\n",
    "\n",
    "        elif isinstance(valid_set, float):\n",
    "            num_images_valid = int(num_images*valid_set)\n",
    "            num_images_train = num_images - num_images_valid\n",
    "\n",
    "            ind_all = list(range(num_images))\n",
    "            random.shuffle(ind_all)\n",
    "            ind_train = ind_all[0:num_images_train]\n",
    "            ind_valid = ind_all[num_images_train:]\n",
    "\n",
    "            img_file_paths_train = [img_file_paths[ind] for ind in ind_train]\n",
    "            img_file_paths_valid = [img_file_paths[ind] for ind in ind_valid]\n",
    "            \n",
    "        img_file_paths_train = [file.stem for file in img_file_paths_train]\n",
    "        img_file_paths_valid = [file.stem for file in img_file_paths_valid]\n",
    "            \n",
    "        return img_file_paths_train, img_file_paths_valid\n",
    "        \n",
    "    def as_tensor(self):\n",
    "        '''Converts all images in the dataset to a single torch tensor.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tensors : torch.Tensor\n",
    "            Tensor with dimensions (num images, num channels, height, width)\n",
    "        '''\n",
    "            \n",
    "        img_file_paths = self.img_file_paths\n",
    "        ret_vals = self.__getitem__(0)           # Open first image to get shape\n",
    "        \n",
    "        num_tensors = len(img_file_paths)\n",
    "        tensors = [torch.zeros((num_tensors, *val.shape), dtype=val.dtype) for val in ret_vals]\n",
    "        \n",
    "        for file_idx, img_file_path in enumerate(img_file_paths):\n",
    "            ret_vals = self.__getitem__(file_idx)\n",
    "            for idx, ret_val in enumerate(ret_vals):\n",
    "                tensors[idx][file_idx] = ret_val\n",
    "\n",
    "        return tensors\n",
    "        \n",
    "    def apply_transforms(self, transforms, img, label, weight=None):\n",
    "        '''Apply transformations stored in transforms\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        transforms : list of functions\n",
    "            List of functions to be applied for image augmentation. Each function should have the signature\n",
    "            transform(img, label, weight=None) and return a tuple (img, label) in case weight is None \n",
    "            or (img, label, weight) otherwise.    \n",
    "        img, label, weight : Image-like\n",
    "            Images to be processed\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        vals : Image-like\n",
    "            Resulting images. Either (img, label) if weight is None or (img, label, weight) otherwise\n",
    "        '''\n",
    "        \n",
    "        if weight is None:\n",
    "            vals = [img, label]\n",
    "        else:\n",
    "            vals = [img, label, weight]\n",
    "        for transform in transforms: \n",
    "                vals = transform(*vals)\n",
    "        return vals\n",
    "    \n",
    "class ImageItem:\n",
    "    \n",
    "    def __init__(self, img, label=None, weight=None):\n",
    "        \n",
    "        self.img = img\n",
    "        self.label = label\n",
    "        self.weight = weight\n",
    "        \n",
    "    def update_img(self, img): self.img = img\n",
    "    def update_label(self, label): self.label = label\n",
    "    def update_weight(self, weight): self.weight = weight\n",
    "    def get_img(self): return self.img\n",
    "    def get_label(self): return self.label\n",
    "    def get_weight(self): return self.weight\n",
    "    def get_items(self): return self.img, self.label, self.weight\n",
    "    def get_defined_items(self): \n",
    "        '''Return items that are not None in a list. If only the image has been defined, return\n",
    "        the image (not a list of one item).'''\n",
    "        \n",
    "        ret = [self.img]\n",
    "        if self.label is not None: ret.append(self.label)\n",
    "        if self.weight is not None: ret.append(self.weight)\n",
    "        if len(ret)==1:\n",
    "            return ret[0]\n",
    "        else:\n",
    "            return ret\n",
    "    \n",
    "    def has_label(self): return self.label is not None\n",
    "    def has_weight(self): return self.weight is not None\n",
    "    \n",
    "    def apply_function(self, func, return_values=False, **kwargs):\n",
    "        \n",
    "        self.img = func(self.img, **kwargs)\n",
    "        if self.label is not None: self.label = func(self.label, **kwargs)\n",
    "        if self.weight is not None: self.weight = func(self.weight, **kwargs)\n",
    "\n",
    "        if return_values:\n",
    "            ret = [self.img]\n",
    "            if self.label is not None: ret.append(self.label)\n",
    "            if self.weight is not None: ret.append(self.weight)\n",
    "            return ret\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": [
     211,
     218,
     228,
     239,
     267,
     274
    ]
   },
   "outputs": [],
   "source": [
    "# Patchwise image dataset storage\n",
    "#export imagedataset.py\n",
    "'''\n",
    "Patchwise image dataset storage\n",
    "'''\n",
    "\n",
    "class ImagePatchDataset(ImageDataset):\n",
    "    \n",
    "    def __init__(self, patch_size, *args, stride=None, patch_transforms=None, **kwargs):\n",
    "    \n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        if stride is None:\n",
    "            stride = patch_size\n",
    "        if isinstance(stride, int):\n",
    "            stride = (stride,)*len(patch_size)\n",
    "        if patch_transforms is None:\n",
    "            patch_transforms = []\n",
    "            \n",
    "        if len(patch_size)!=len(stride):\n",
    "            raise ValueError('`patch_size` and `stride` must have same length')\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.stride = stride\n",
    "        self.patch_transforms = patch_transforms\n",
    "        \n",
    "        self.generate_patches_corners_for_dataset()\n",
    "   \n",
    "    def __getitem__(self, idx):\n",
    "        '''Returns one item from the dataset. Will return an image and label if weight_func was\n",
    "        not defined during class instantiation or an aditional weight image otherwise.'''\n",
    "        \n",
    "        img_index, patch_corners = self.patches_corners[idx]\n",
    "        ret = super().__getitem__(img_index)\n",
    "        if self.weight_func is None:\n",
    "            img, label = ret\n",
    "        else:\n",
    "            img, label, weight = ret\n",
    "        \n",
    "        #first_row, first_col, last_row, last_col = patch_corners\n",
    "        img_patch = self.crop_img(img, patch_corners)\n",
    "        label_patch = self.crop_img(label, patch_corners)\n",
    "            \n",
    "        if self.weight_func is not None:\n",
    "            weight_patch = self.crop_img(weight, patch_corners)\n",
    "            # Apply transforms on patch\n",
    "            ret_transf = self.apply_transforms(self.patch_transforms, img_patch, label_patch, weight_patch)\n",
    "        else:\n",
    "            ret_transf = self.apply_transforms(self.patch_transforms, img_patch, label_patch)        \n",
    "              \n",
    "        return ret_transf\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.patches_corners)\n",
    "    \n",
    "    def subset(self, filename_filter):\n",
    "        \n",
    "        img_dir = copy.copy(self.img_dir)\n",
    "        label_dir = copy.copy(self.label_dir)\n",
    "        name_2_label_map = self.name_2_label_map\n",
    "        img_opener = self.img_opener\n",
    "        label_opener = self.label_opener\n",
    "        transforms = copy.copy(self.transforms)\n",
    "        weight_func = self.weight_func\n",
    "        patch_size = copy.copy(self.patch_size)\n",
    "        stride = copy.copy(self.stride)\n",
    "        patch_transforms = copy.copy(self.patch_transforms)\n",
    "        return self.__class__(patch_size, img_dir, label_dir, name_2_label_map, stride=stride,\n",
    "                              filename_filter=filename_filter, img_opener=img_opener, label_opener=label_opener, \n",
    "                              transforms=transforms, patch_transforms=patch_transforms, weight_func=weight_func)\n",
    "       \n",
    "    def generate_patches_corners_for_dataset(self, img_shape=None):\n",
    "        '''If img_shape is None, generates indices by opening each image to get the\n",
    "        respective shape. This is useful when images have distinct sizes. If img_shape\n",
    "        is not None, uses that shape and the images are not opened, which is much faster.'''\n",
    "     \n",
    "        if img_shape is None:\n",
    "            must_open = True\n",
    "        else:\n",
    "            must_open = False\n",
    "\n",
    "        self.patches_corners = []\n",
    "        self.patch_index_accumulator = [0]\n",
    "        for img_idx, img_file_path in enumerate(self.img_file_paths):\n",
    "            if must_open:\n",
    "                try:\n",
    "                    ret = super().__getitem__(img_idx)\n",
    "                except Exception:    \n",
    "                    raise Exception(f'Cannot get image {img_file_path}\\n')\n",
    "                img_shape = self.get_shape(ret[0])\n",
    "                if len(self.patch_size)!=len(img_shape):\n",
    "                    raise ValueError('Length of `patch_size` must be the same as image dimension')\n",
    "            patches_corners_img = self.generate_patches_corners_for_image(self.patch_size, self.stride, img_shape)\n",
    "            self.patches_corners.extend(zip([img_idx]*len(patches_corners_img), patches_corners_img))\n",
    "            \n",
    "    def generate_patches_corners_for_image(self, patch_size, stride, img_shape):\n",
    "            \n",
    "        if len(img_shape)==2:\n",
    "            patches_corners = self.generate_patches_corners_for_2d_image(patch_size, stride, img_shape)\n",
    "        elif len(img_shape)==3:\n",
    "            patches_corners = self.generate_patches_corners_for_3d_image(patch_size, stride, img_shape)\n",
    "        else:\n",
    "            raise Exception('Image must be 2D or 3D')\n",
    "            \n",
    "        return patches_corners\n",
    "    \n",
    "    def generate_patches_corners_for_2d_image(self, patch_size, stride, img_shape):\n",
    "            \n",
    "        #patch_count = 0\n",
    "        patches_corners = []\n",
    "        for row in range(0, img_shape[0]-patch_size[0]+stride[0], stride[0]):\n",
    "            if (row+patch_size[0])>=img_shape[0]:\n",
    "                # Do not go over image border\n",
    "                row = img_shape[0] - patch_size[0]\n",
    "            for col in range(0, img_shape[1]-patch_size[1]+stride[1], stride[1]):\n",
    "                if (col+patch_size[1])>=img_shape[1]:\n",
    "                    # Do not go over image border\n",
    "                    col = img_shape[1] - patch_size[1]\n",
    "                    \n",
    "                patch_corners = (slice(row, row+patch_size[0]), slice(col, col+patch_size[1]))\n",
    "                patches_corners.append(patch_corners)\n",
    "                #patch_count += 1\n",
    "        #self.patch_index_accumulator.append(self.patch_index_accumulator[-1] + patch_count)\n",
    "        return patches_corners\n",
    "\n",
    "    def generate_patches_corners_for_3d_image(self, patch_size, stride, img_shape):\n",
    "            \n",
    "        #patch_count = 0\n",
    "        patches_corners = []\n",
    "        for plane in range(0, img_shape[0]-patch_size[0]+stride[0], stride[0]):\n",
    "            if (plane+patch_size[0])>=img_shape[0]:\n",
    "                # Do not go over image border\n",
    "                plane = img_shape[0] - patch_size[0]\n",
    "            for row in range(0, img_shape[1]-patch_size[1]+stride[1], stride[1]):\n",
    "                if (row+patch_size[1])>=img_shape[1]:\n",
    "                    # Do not go over image border\n",
    "                    row = img_shape[1] - patch_size[1]\n",
    "                for col in range(0, img_shape[2]-patch_size[2]+stride[2], stride[2]):\n",
    "                    if (col+patch_size[2])>=img_shape[2]:\n",
    "                        # Do not go over image border\n",
    "                        col = img_shape[2] - patch_size[2]\n",
    "\n",
    "                    patch_corners = (slice(plane, plane+patch_size[0]), slice(row, row+patch_size[1]),\n",
    "                                     slice(col, col+patch_size[2]))\n",
    "                    patches_corners.append(patch_corners)\n",
    "\n",
    "        return patches_corners\n",
    "       \n",
    "    def get_patch_from_index(self, index, img_shape):\n",
    "        pass\n",
    "              \n",
    "    def get_shape(self, img, warn=True):\n",
    "        \n",
    "        if isinstance(img, Image.Image):\n",
    "            img_shape = (img.height, img.width)\n",
    "        elif isinstance(img, torch.Tensor):\n",
    "            img_shape = img.shape     \n",
    "            if (img.ndim==3): \n",
    "                if img_shape[-3]<=3:\n",
    "                    # Consider that third to last dimension is for color\n",
    "                    img_shape = img_shape[-2:]\n",
    "                else:\n",
    "                    img_shape = img_shape[-3:]\n",
    "            if (img.ndim==4): \n",
    "                img_shape = img_shape[-3:]\n",
    "        elif isinstance(img, np.ndarray):\n",
    "            img_shape = img.shape    \n",
    "            if img.ndim==3: \n",
    "                if img_shape[-1]<=3:\n",
    "                    # Consider that last dimension is for color\n",
    "                    img_shape = img_shape[-3:-1]\n",
    "                else:\n",
    "                    img_shape = img_shape[-3:]\n",
    "            elif img.ndim==4:\n",
    "                img_shape = img_shape[-3:]\n",
    "        else:\n",
    "            raise AttributeError(\"Image is not a PIL, Tensor or ndarray. Cannot safely infer shape\")\n",
    "            \n",
    "        if min(img_shape)<=3:\n",
    "            print(f'Warning, inferred shape {img_shape} is probably incorrect. Sizes smaller than 4 are being discarded')\n",
    "            img_shape = filter(lambda v:v>3, img_shape)\n",
    "    \n",
    "        return img_shape\n",
    "    \n",
    "    def crop_img(self, img, patch_corners):\n",
    "        '''TODO: Decide if we should include imgaug crop. Otherwise crop will not work if last\n",
    "        transform of self.img_transforms is from imgaug.'''\n",
    "        \n",
    "        if isinstance(img, Image.Image):\n",
    "            first_row, last_row = patch_corners[0].start, patch_corners[0].stop\n",
    "            first_col, last_col = patch_corners[1].start, patch_corners[1].stop\n",
    "            img_patch = img.crop([first_col, first_row, last_col, last_row])\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            # Crop from trailing dimensions\n",
    "            img_patch = img[(...,)+patch_corners]\n",
    "        else:\n",
    "            try:\n",
    "                img_patch = img[patch_corners]\n",
    "            except Exception:\n",
    "                raise IndexError(f'Cannot crop image of type {type(img)}')\n",
    "                \n",
    "        #import pdb; pdb.set_trace()\n",
    "        \n",
    "        return img_patch\n",
    "    \n",
    "    @classmethod\n",
    "    def get_image_from_patches(cls, patches, stride, img_shape, operation='max'):\n",
    "        \n",
    "        # Remove single channel dimension\n",
    "        if patches[0].ndim==4:\n",
    "            is_3d = True\n",
    "        else:\n",
    "            is_3d = False\n",
    "\n",
    "        if not isinstance(patches[0], torch.Tensor):\n",
    "            patches = [transforms.transf_to_tensor(patch, is_3d=is_3d) for patch in patches]\n",
    "            \n",
    "        #if patches[0].shape[0]==1:\n",
    "        #    patches = [patch[0] for patch in patches]\n",
    "            \n",
    "        patch_size = patches[0].shape[1:]\n",
    "        img = torch.zeros(img_shape, dtype=patches[0].dtype)\n",
    "        patches_corners = cls.generate_patches_corners_for_image(patch_size, stride, img_shape)\n",
    "        \n",
    "        if operation=='mean':\n",
    "            img_count = torch.zeros(img_shape, dtype=int)\n",
    "        for patch_corners, patch in zip(patches_corners, patches):\n",
    "            img_patch = img[patch_corners]\n",
    "            if operation=='max':\n",
    "                img_patch[:] = torch.where(img_patch>patch, img_patch, patch)\n",
    "            elif operation=='min':\n",
    "                img_patch[:] = torch.where(img_patch<patch, img_patch, patch)\n",
    "            elif operation=='mean':\n",
    "                img_patch[:] = img_patch + patch\n",
    "                img_count[patch_corners] += 1\n",
    "                \n",
    "        if operation=='mean':\n",
    "            mask = img_count>0\n",
    "            img[mask] = img[mask]/img_count[mask]\n",
    "            \n",
    "        return img\n",
    "                \n",
    "    # Functions with some ideas, not necessary for class\n",
    "    def crop_2d_img(self, img, patch_corners):\n",
    "        \n",
    "        first_row, first_col, last_row, last_col = patch_corners\n",
    "        if isinstance(img, Image.Image):\n",
    "            img_patch = img.crop([first_col, first_row, last_col, last_row])\n",
    "        else:\n",
    "            img_patch = img[first_row:last_row, first_col:last_col]\n",
    "\n",
    "        return img_patch\n",
    "    \n",
    "    def crop_3d_img(self, img, patch_corners):\n",
    "        \n",
    "        first_plane, first_row, first_col, last_plane, last_row, last_col = patch_corners\n",
    "        \n",
    "        if isinstance(img, Image.Image):\n",
    "            raise ValueError('Cannot interpret PIL image as 3D')\n",
    "        else:\n",
    "            img_patch = img[first_plane:last_plane, first_row:last_row, first_col:last_col]\n",
    "            \n",
    "        return img_patch\n",
    "                                 \n",
    "    def generate_patch_index_accumulator(self, patch_size, stride=None, img_shape=None):\n",
    "        '''If img_shape is None, generates indices by opening each image to get the\n",
    "        respective shape. This is useful when images have distinct sizes. If img_shape\n",
    "        is not None, uses that shape and the images are not opened, which is much faster.'''\n",
    "     \n",
    "        if img_shape is None:\n",
    "            must_open = True\n",
    "        else:\n",
    "            must_open = False\n",
    "\n",
    "        index_accumulator = [0]\n",
    "        img_shapes = []\n",
    "        for img_file_path in self.img_file_paths:\n",
    "            if must_open:\n",
    "                try:\n",
    "                    img = self.img_opener(img_file_path)\n",
    "                except Exception:    \n",
    "                    raise Exception(f'Cannot open image {img_file_path}\\n')\n",
    "                \n",
    "            img_shape = self.get_shape(img)\n",
    "            num_patches = self._num_patches_in_img(patch_size, stride, img_shape)\n",
    "            \n",
    "            img_shapes.append(img_shape)\n",
    "            index_accumulator.append(index_accumulator[-1] + num_patches)\n",
    "        \n",
    "        self.img_shapes = img_shapes\n",
    "        self.patch_index_accumulator = index_accumulator\n",
    "                \n",
    "    def get_patch_from_global_index(self, index):\n",
    "        \n",
    "        img_index = bisect.bisect(self.patch_index_accumulator, index) - 1\n",
    "        img_shape = self.img_shapes[img_index]\n",
    "        patch_index = index - self.patch_index_accumulator[img_index]\n",
    "        get_patch_from_index(self, patch_index, img_shape)      \n",
    "        \n",
    "    def _num_patches_in_img(self, patch_size, stride, img_shape):\n",
    "        \n",
    "        num_p_rows = (img_shape[0]-patch_size[0])//stride[0] + 1\n",
    "        if num_p_rows*stride[0]!=img_shape[0]:\n",
    "            # If patches do not fit perfectly\n",
    "            num_p_rows += 1\n",
    "        num_p_cols = (img_shape[1]-patch_size[1])//stride[1] + 1\n",
    "        if num_p_cols*stride[1]!=img_shape[1]:\n",
    "            # If patches do not fit perfectly\n",
    "            num_p_cols += 1\n",
    "            \n",
    "        return num_p_rows*num_p_cols\n",
    "                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [
     0,
     10,
     30
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All images read\n"
     ]
    }
   ],
   "source": [
    "# Create dataloaders\n",
    "\n",
    "from pathlib import Path\n",
    "from torchtrainer import img_util\n",
    "from torchtrainer import transforms\n",
    "import imgaug.augmenters as iaa\n",
    "import re\n",
    "from torch.utils.data import dataloader as torch_dataloader\n",
    "from functools import partial\n",
    "\n",
    "def name_2_label_map(img_filename):\n",
    "    '''Maps image names to labels names in the DRIVE dataset\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    img_filename : string\n",
    "        Filename of an image\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    label_filename : string\n",
    "        Filename of the corresponding label image\n",
    "    '''  \n",
    "    \n",
    "    m = re.match('(\\d\\d)_[training|test]', img_filename)\n",
    "    index = m.group(1)\n",
    "    label_filename = index + '_manual1.gif'\n",
    "    \n",
    "    return label_filename\n",
    "\n",
    "def filename_filter(img_filename, test=False):\n",
    "    '''Filter DRIVE images for getting only training or test images.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    img_filename : string\n",
    "        Filename of an image\n",
    "    test : bool\n",
    "        If True, the function checks if the image should be in the test set\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        If `test` is False, returns True if the image should be in the training set and\n",
    "        False if it should be in the test set. Return values are reversed if `test` is True\n",
    "    '''\n",
    "    \n",
    "    if test:\n",
    "        return 'test' in img_filename\n",
    "    else:\n",
    "        return 'test' not in img_filename\n",
    "\n",
    "root_dir = Path('../drive/data/DRIVE')\n",
    "img_dir = root_dir/'images'\n",
    "label_dir = root_dir/'labels'\n",
    "mask_dir = root_dir/'mask'\n",
    "\n",
    "# Create functions for using in the dataset creation\n",
    "img_opener_partial = partial(img_util.pil_img_opener, channel=None)\n",
    "label_opener_partial = partial(img_util.pil_img_opener, is_label=True)\n",
    "\n",
    "# Image transformations\n",
    "imgaug_seq = iaa.Sequential([\n",
    "                            iaa.Resize({\"height\": 560, \"width\": 560}),\n",
    "                            ])\n",
    "transform_funcs = transforms.seq_pil_to_imgaug_to_tensor(imgaug_seq)\n",
    "\n",
    "# Create ImageDataset instance\n",
    "dataset = ImageDataset(img_dir, label_dir, name_2_label_map=name_2_label_map, filename_filter=filename_filter, \n",
    "                       img_opener=img_opener_partial, label_opener=label_opener_partial, transforms=transform_funcs)\n",
    "\n",
    "dataset.check_dataset()\n",
    "\n",
    "train_ds, valid_ds = dataset.split_train_val(0.2)\n",
    "\n",
    "# We can grab all images as tensor\n",
    "x_train,y_train = train_ds.as_tensor()\n",
    "x_valid,y_valid = valid_ds.as_tensor()\n",
    "\n",
    "# Or create a dataloader\n",
    "train_dl = torch_dataloader.DataLoader(train_ds, batch_size=4, shuffle=True)\n",
    "valid_dl = torch_dataloader.DataLoader(valid_ds, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def img_opener(img_file_path, is_label=False):\n",
    "    \n",
    "    if is_label: \n",
    "        img = torch.randint(0, 2, (50, 500, 500)) #np.random.randint(0, 2, (50, 500, 500))\n",
    "    else:\n",
    "        img = torch.randint(0, 256, (50, 500, 500)) #np.random.randint(0, 256, (50, 500, 500))\n",
    "        \n",
    "    return img\n",
    "\n",
    "patch_size = (50, 100, 100)\n",
    "stride = (50, 50, 50)\n",
    "\n",
    "\n",
    "imgaug_seq = iaa.Sequential([\n",
    "                            iaa.Resize({\"height\": 500, \"width\": 500}),\n",
    "                            ])\n",
    "img_transform_funcs = [] #transforms.seq_pil_to_imgaug_to_tensor(imgaug_seq)\n",
    "\n",
    "# Patch transformations\n",
    "imgaug_seq = iaa.Sequential([\n",
    "                            iaa.Resize({\"height\": 120, \"width\": 120}),\n",
    "                            ])\n",
    "patch_transform_funcs = [partial(transforms.transf_to_tensor, is_3d=False), transforms.transf_normalize] \n",
    "                           #[transforms.transf_to_imgaug, transforms.translate_imagaug_seq(imgaug_seq), \n",
    "                           #transforms.transf_to_tensor]\n",
    "\n",
    "# Create ImageDataset instance\n",
    "dataset = ImagePatchDataset(patch_size, img_dir, label_dir, name_2_label_map=name_2_label_map, filename_filter=filename_filter,\n",
    "                            img_opener=img_opener, label_opener=partial(img_opener, True), \n",
    "                            transforms=img_transform_funcs, patch_transforms=patch_transform_funcs, stride=stride)\n",
    "\n",
    "#dataset.check_dataset()\n",
    "\n",
    "train_ds, valid_ds = dataset.split_train_val(0.2)\n",
    "\n",
    "# We can grab all images as tensor\n",
    "x_train,y_train = train_ds.as_tensor()\n",
    "x_valid,y_valid = valid_ds.as_tensor()\n",
    "\n",
    "# Or create a dataloader\n",
    "train_dl = torch_dataloader.DataLoader(train_ds, batch_size=4, shuffle=True)\n",
    "valid_dl = torch_dataloader.DataLoader(valid_ds, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1;32m<ipython-input-19-c861eb4ac9e5>\u001b[0m(266)\u001b[0;36m<listcomp>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m    264 \u001b[1;33m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    265 \u001b[1;33m        \u001b[0mnum_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_file_paths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m--> 266 \u001b[1;33m        \u001b[0mtensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mret_vals\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    267 \u001b[1;33m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    268 \u001b[1;33m        \u001b[1;32mfor\u001b[0m \u001b[0mfile_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_file_path\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_file_paths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> num_tensors\n",
      "16\n",
      "ipdb> val.shape\n",
      "(100, 100, 50)\n",
      "ipdb> (num_tensors, *val.shape)\n",
      "(16, 100, 100, 50)\n",
      "ipdb> torch.zeros((num_tensors, *val.shape), dtype=val.dtype)\n",
      "*** TypeError: zeros() received an invalid combination of arguments - got (tuple, dtype=numpy.dtype), but expected one of:\n",
      " * (tuple of ints size, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
      " * (tuple of ints size, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
      "ipdb> torch.zeros((num_tensors, *val.shape))\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipdb> val.dtype\n",
      "dtype('float64')\n",
      "ipdb> torch.zeros((num_tensors, *val.shape), dtype=val.dtype)\n",
      "*** TypeError: zeros() received an invalid combination of arguments - got (tuple, dtype=numpy.dtype), but expected one of:\n",
      " * (tuple of ints size, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
      " * (tuple of ints size, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
      "ipdb> torch.zeros((5, 6, 4, 6), dtype=val.dtype)\n",
      "*** TypeError: zeros() received an invalid combination of arguments - got (tuple, dtype=numpy.dtype), but expected one of:\n",
      " * (tuple of ints size, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
      " * (tuple of ints size, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
      "ipdb> torch.zeros((5, 6, 4), dtype=val.dtype)\n",
      "*** TypeError: zeros() received an invalid combination of arguments - got (tuple, dtype=numpy.dtype), but expected one of:\n",
      " * (tuple of ints size, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
      " * (tuple of ints size, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
      "ipdb> torch.zeros((5, 6, 4, 6))\n",
      "tensor([[[[0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0.]]]])\n",
      "ipdb> torch.zeros(6, dtype=val.dtype)\n",
      "*** TypeError: zeros() received an invalid combination of arguments - got (int, dtype=numpy.dtype), but expected one of:\n",
      " * (tuple of ints size, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
      " * (tuple of ints size, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
      "ipdb> val.dtype\n",
      "dtype('float64')\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8820"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = iaa.Resize({\"height\": 120, \"width\": 120})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Got an unknown datatype for argument 'segmentation_maps'. Expected datatypes were: None, array[int], array[uint], array[bool], SegmentationMapsOnImage, iterable[empty], iterable-array[int], iterable-array[uint], iterable-array[bool], iterable-SegmentationMapsOnImage. Got: str.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-ce981ddb714d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msegmentation_maps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\Miniconda3\\lib\\site-packages\\imgaug\\augmenters\\meta.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m         \u001b[1;34m\"\"\"Alias for :func:`~imgaug.augmenters.meta.Augmenter.augment`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maugment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprocesses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxtasksperchild\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Miniconda3\\lib\\site-packages\\imgaug\\augmenters\\meta.py\u001b[0m in \u001b[0;36maugment\u001b[1;34m(self, return_batch, hooks, **kwargs)\u001b[0m\n\u001b[0;32m   1977\u001b[0m         )\n\u001b[0;32m   1978\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1979\u001b[1;33m         \u001b[0mbatch_aug\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maugment_batch_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1980\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1981\u001b[0m         \u001b[1;31m# return either batch or tuple of augmentables, depending on what\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Miniconda3\\lib\\site-packages\\imgaug\\augmenters\\meta.py\u001b[0m in \u001b[0;36maugment_batch_\u001b[1;34m(self, batch, parents, hooks)\u001b[0m\n\u001b[0;32m    594\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUnnormalizedBatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m             \u001b[0mbatch_unnorm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m             \u001b[0mbatch_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_normalized_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    597\u001b[0m             \u001b[0mbatch_inaug\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_norm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_batch_in_augmentation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Miniconda3\\lib\\site-packages\\imgaug\\augmentables\\batches.py\u001b[0m in \u001b[0;36mto_normalized_batch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m                 self.heatmaps_unaug, shapes),\n\u001b[0;32m    205\u001b[0m             segmentation_maps=nlib.normalize_segmentation_maps(\n\u001b[1;32m--> 206\u001b[1;33m                 self.segmentation_maps_unaug, shapes),\n\u001b[0m\u001b[0;32m    207\u001b[0m             keypoints=nlib.normalize_keypoints(\n\u001b[0;32m    208\u001b[0m                 self.keypoints_unaug, shapes),\n",
      "\u001b[1;32mE:\\Miniconda3\\lib\\site-packages\\imgaug\\augmentables\\normalization.py\u001b[0m in \u001b[0;36mnormalize_segmentation_maps\u001b[1;34m(inputs, shapes)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_preprocess_shapes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m     \u001b[0mntype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimate_segmaps_norm_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m     _assert_exactly_n_shapes_partial = functools.partial(\n\u001b[0;32m    197\u001b[0m         \u001b[0m_assert_exactly_n_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Miniconda3\\lib\\site-packages\\imgaug\\augmentables\\normalization.py\u001b[0m in \u001b[0;36mestimate_segmaps_norm_type\u001b[1;34m(segmentation_maps)\u001b[0m\n\u001b[0;32m   1085\u001b[0m     ]\n\u001b[0;32m   1086\u001b[0m     _assert_is_of_norm_type(\n\u001b[1;32m-> 1087\u001b[1;33m         type_str, valid_type_strs, \"segmentation_maps\")\n\u001b[0m\u001b[0;32m   1088\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtype_str\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1089\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Miniconda3\\lib\\site-packages\\imgaug\\augmentables\\normalization.py\u001b[0m in \u001b[0;36m_assert_is_of_norm_type\u001b[1;34m(type_str, valid_type_strs, arg_name)\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[1;34m\"Got an unknown datatype for argument '%s'. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1054\u001b[0m         \"Expected datatypes were: %s. Got: %s.\" % (\n\u001b[1;32m-> 1055\u001b[1;33m             arg_name, \", \".join(valid_type_strs), type_str))\n\u001b[0m\u001b[0;32m   1056\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Got an unknown datatype for argument 'segmentation_maps'. Expected datatypes were: None, array[int], array[uint], array[bool], SegmentationMapsOnImage, iterable[empty], iterable-array[int], iterable-array[uint], iterable-array[bool], iterable-SegmentationMapsOnImage. Got: str."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "img = np.zeros([30,30])\n",
    "res = r(image=img, segmentation_maps='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'transforms' from 'torchtrainer' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-0843b4a74dd6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchtrainer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'transforms' from 'torchtrainer' (unknown location)"
     ]
    }
   ],
   "source": [
    "from torchtrainer import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " O volume na unidade E  Arquivos\n",
      " O Nmero de Srie do Volume  72F7-290B\n",
      "\n",
      " Pasta de E:\\Dropbox\n",
      "\n",
      "23/04/2020  12:17    <DIR>          .\n",
      "23/04/2020  12:17    <DIR>          ..\n",
      "19/12/2019  11:44                39 .dropbox\n",
      "06/10/2019  17:44    <DIR>          .ipynb_checkpoints\n",
      "05/07/2019  11:36         8.062.114 1-s2.0-S037015731000308X-main.pdf\n",
      "23/04/2020  11:42     2.739.976.424 12 mo wildtype mice.tar.gz\n",
      "11/07/2019  08:53       651.464.008 6 mo AD mice-20190711T114819Z-001.zip\n",
      "20/11/2015  20:17    <DIR>          afarewelltocolor\n",
      "03/02/2020  11:33    <DIR>          apartamento\n",
      "04/01/2017  19:24    <DIR>          Aplicativos\n",
      "08/08/2018  18:42    <DIR>          Apresentacoes\n",
      "11/01/2016  13:42    <DIR>          arbitros\n",
      "19/12/2019  11:48    <DIR>          ArcGis\n",
      "28/01/2019  14:57    <DIR>          ArcGIS_10.4.1\n",
      "19/12/2019  12:13    <DIR>          article\n",
      "21/12/2019  18:53    <DIR>          artigoNeurons\n",
      "20/11/2015  20:20    <DIR>          Artigos\n",
      "27/09/2015  21:23               558 arvore.py\n",
      "20/11/2015  20:20    <DIR>          arxiv_crawler\n",
      "20/11/2015  20:20    <DIR>          Aula Iglu\n",
      "19/12/2019  11:48    <DIR>          Autism project\n",
      "19/12/2019  11:48    <DIR>          Axon_Orientations\n",
      "04/07/2016  20:34    <DIR>          backups\n",
      "18/11/2016  09:05           581.625 Baptiste article.pdf\n",
      "21/08/2019  18:00       921.161.884 Baptiste data.zip\n",
      "20/11/2015  20:23    <DIR>          base\n",
      "20/11/2015  20:23    <DIR>          Benchmark SF networks\n",
      "24/05/2019  15:16        11.772.909 binary.zip\n",
      "26/10/2013  15:25               412 blocks.py\n",
      "19/12/2019  12:13    <DIR>          bones\n",
      "19/12/2019  11:48    <DIR>          Bones_2\n",
      "19/12/2019  12:13    <DIR>          bone_channels\n",
      "05/10/2019  16:16        43.399.391 bone_small.tif\n",
      "19/12/2019  11:48    <DIR>          BostonFilipi\n",
      "19/12/2019  11:48    <DIR>          BrainNetSporns\n",
      "02/05/2019  13:09    <DIR>          Bruna\n",
      "20/11/2015  20:24    <DIR>          C++\n",
      "28/05/2018  17:17    <DIR>          cam\n",
      "02/04/2020  21:26    <DIR>          Camera Uploads\n",
      "02/06/2019  09:51    <DIR>          Cartas\n",
      "26/02/2019  18:31    <DIR>          Certificados\n",
      "22/12/2019  00:12    <DIR>          cesar\n",
      "21/06/2019  13:41    <DIR>          cffi\n",
      "20/11/2015  20:24    <DIR>          CFinder-2.0.5--1440\n",
      "19/04/2019  12:38             1.765 ChangeProofingLanguage.txt\n",
      "09/10/2013  14:03                29 chave.txt\n",
      "19/12/2019  11:48    <DIR>          CidadesGuilherme\n",
      "19/12/2019  11:48    <DIR>          CityAttack\n",
      "14/02/2020  18:39    <DIR>          codigos\n",
      "19/12/2019  12:13    <DIR>          comparacoes_arXiv\n",
      "24/03/2020  13:50        17.672.643 Complex_systems__features__similarity_and_connectivity.pdf\n",
      "22/08/2017  09:17           570.783 comprovantes.pdf\n",
      "12/01/2017  20:23    <DIR>          concursos\n",
      "13/11/2016  11:19    <DIR>          config\n",
      "06/08/2016  13:33    <DIR>          crawlers\n",
      "07/05/2019  21:15            19.284 create_tube_v4.py\n",
      "25/06/2019  20:36    <DIR>          cython\n",
      "19/12/2019  11:48    <DIR>          Cncer\n",
      "11/04/2020  09:49    <DIR>          deep learning\n",
      "19/12/2019  11:48    <DIR>          DensityResultsShared\n",
      "03/09/2019  20:53        19.095.499 desenho tcnico.pdf\n",
      "22/12/2015  14:50    <DIR>          Desktop\n",
      "29/06/2019  15:47    <DIR>          detectron\n",
      "19/12/2019  11:48    <DIR>          Development project\n",
      "19/12/2019  12:13    <DIR>          dinamicaRumores\n",
      "19/12/2019  11:48    <DIR>          Disciplinas BCC_ENC - 2018\n",
      "21/12/2019  10:00    <DIR>          Dout\n",
      "19/10/2019  18:31    <DIR>          Doutorado\n",
      "28/09/2014  21:58             9.303 dungeons.png\n",
      "26/03/2016  11:28    <DIR>          easynvest\n",
      "20/11/2015  20:24    <DIR>          Escrita cientfica\n",
      "19/12/2019  11:48    <DIR>          EvennessNetworks\n",
      "09/04/2019  11:25     2.253.876.179 Experiment #1 (adults set #1)_20x_batch1 - Layer IV.zip\n",
      "25/03/2020  16:33     1.677.445.120 felix.tar\n",
      "27/04/2014  16:04               315 file_shuffler.py\n",
      "19/12/2019  11:48    <DIR>          Financial_Market\n",
      "14/06/2019  22:33    <DIR>          flask\n",
      "19/12/2019  13:55    <DIR>          fly_tracking\n",
      "19/12/2019  11:48    <DIR>          For 2D_3D\n",
      "27/06/2019  08:27            65.057 formularios-de-solicitacao-de-ferias-servidor_fev_2019editavel.docx\n",
      "27/06/2019  08:33           239.499 formularios-de-solicitacao-de-ferias-servidor_fev_2019editavel.pdf\n",
      "19/12/2019  11:48    <DIR>          FutebolUnicamp\n",
      "20/11/2015  20:26    <DIR>          HM_crawler\n",
      "19/12/2019  13:55    <DIR>          imagensGeradas\n",
      "20/09/2019  08:57       263.902.877 imagens_corrompidas.zip\n",
      "19/12/2019  11:48    <DIR>          Images\n",
      "10/11/2019  15:15    <DIR>          Images for Cesar-9\n",
      "05/10/2016  17:15             1.718 imported_modules.py\n",
      "19/12/2019  11:48    <DIR>          Imprimi pra mim\n",
      "11/03/2020  18:34    <DIR>          infos\n",
      "20/11/2015  20:27    <DIR>          inner myelin detection\n",
      "20/11/2015  20:27    <DIR>          invest\n",
      "20/10/2011  20:46                52 IP.txt\n",
      "13/09/2013  14:22               114 IPs.txt\n",
      "16/06/2019  14:57    <DIR>          javascript\n",
      "21/09/2016  09:44    <DIR>          jupyter\n",
      "10/10/2019  18:07    <DIR>          jupyterhub\n",
      "10/11/2017  16:29             3.600 LaTeX.js\n",
      "19/12/2019  13:55    <DIR>          manuscriptTwitter\n",
      "22/12/2019  00:49    <DIR>          manuscript_symmetry\n",
      "19/12/2019  11:48    <DIR>          Mapeamentos\n",
      "20/11/2015  20:27    <DIR>          medidas\n",
      "12/06/2018  20:19    <DIR>          mestrado\n",
      "09/02/2019  09:20    <DIR>          Meus artigos\n",
      "19/12/2019  11:48    <DIR>          MEV- CA 19-9\n",
      "04/10/2019  12:05         9.648.108 Montage.png\n",
      "19/12/2019  12:02    <DIR>          Motifs\n",
      "13/10/2019  11:32           307.748 nc2-atrasos-e-nao-entregues.pdf\n",
      "20/11/2015  20:33    <DIR>          Networks 3D.app\n",
      "24/05/2019  15:16           133.832 network_core.zip\n",
      "19/12/2019  13:55    <DIR>          neuron-classification\n",
      "20/11/2015  20:33    <DIR>          new computer\n",
      "19/12/2019  11:48    <DIR>          New images\n",
      "19/12/2019  11:48    <DIR>          NewPresentations\n",
      "12/04/2012  10:52            15.828 Notas.txt\n",
      "13/06/2018  10:33            13.178 object_detection_tutorial.py\n",
      "14/09/2019  12:02     3.210.803.200 ocs_files.tar\n",
      "20/11/2015  20:33    <DIR>          Outros_codigos\n",
      "22/12/2019  08:39    <DIR>          paper_and_report\n",
      "22/12/2019  01:36    <DIR>          paper_limiar\n",
      "19/12/2019  13:55    <DIR>          paper_relevo\n",
      "15/03/2019  19:00    <DIR>          PDFs\n",
      "19/12/2019  13:55    <DIR>          phasefocus\n",
      "20/11/2015  20:34    <DIR>          piracicaba\n",
      "20/10/2017  13:17         1.413.515 plano de trabalho assinado.pdf\n",
      "14/12/2019  16:21        87.128.218 plexmediaserver_1.18.2.2058-e67a4e892_amd64.deb\n",
      "19/12/2019  11:48    <DIR>          PlosOneViews\n",
      "21/11/2013  12:42             6.952 plota.py\n",
      "21/11/2015  16:07    <DIR>          plotly\n",
      "26/03/2018  21:17    <DIR>          Posd\n",
      "19/12/2019  11:48    <DIR>          Presentations\n",
      "10/08/2017  13:51             1.482 private-key.ppk\n",
      "25/06/2019  10:17               100 projecao coronal sagital e axial\n",
      "09/04/2020  23:05    <DIR>          Project neuroinflammation\n",
      "20/11/2015  20:36    <DIR>          provas exame\n",
      "27/05/2011  21:56           970.299 pt_BR.aff\n",
      "27/05/2011  21:55         4.588.956 pt_BR.dic\n",
      "19/12/2019  11:48    <DIR>          Public\n",
      "20/11/2015  20:36    <DIR>          PyBliss-0.50beta\n",
      "17/06/2017  12:30               614 pyfiles.py\n",
      "20/11/2015  20:37    <DIR>          Python\n",
      "05/10/2017  13:08    <DIR>          Redes Complexas\n",
      "27/11/2019  17:37    <DIR>          referee\n",
      "03/10/2019  09:22            31.098 regioes.png\n",
      "17/09/2019  10:57       662.473.731 Relatrios parciais.zip\n",
      "19/12/2019  13:55    <DIR>          report\n",
      "19/12/2019  14:45    <DIR>          report_to_bavaria\n",
      "19/12/2019  11:48    <DIR>          ResultadosCesar\n",
      "19/12/2019  11:48    <DIR>          Revision Nature Neuroscience\n",
      "15/06/2017  16:13    <DIR>          revistas\n",
      "18/04/2020  09:24    <DIR>          ROIs\n",
      "20/11/2015  20:37    <DIR>          sao jose\n",
      "19/12/2019  11:48    <DIR>          Science\n",
      "03/11/2019  10:45            68.745 Screenshot from 2019-11-03 11-44-54.png\n",
      "22/12/2019  08:39    <DIR>          selenium\n",
      "19/12/2019  11:48    <DIR>          Seminrio_Gonza\n",
      "06/06/2017  21:53    <DIR>          server\n",
      "02/07/2016  18:33    <DIR>          shell\n",
      "25/06/2019  20:36    <DIR>          simplest\n",
      "20/11/2015  20:37    <DIR>          socscibot2pajek_v1.0\n",
      "05/06/2017  13:51        58.299.163 spring_good_images.zip\n",
      "20/11/2015  20:37    <DIR>          steps\n",
      "05/11/2019  21:28    <DIR>          Stroke 3D project\n",
      "19/12/2019  11:48    <DIR>          Stroke project\n",
      "19/12/2019  11:48    <DIR>          Stroke rats\n",
      "20/11/2015  20:37    <DIR>          submitted_manuscript_symmetry\n",
      "30/11/2017  21:38            25.693 Sumula_Curricular_beneficiario.docx\n",
      "30/11/2017  21:38            29.501 Sumula_Curricular_cesar.pdf\n",
      "22/12/2019  08:45    <DIR>          Survey Spatial Networks\n",
      "22/05/2011  12:39            12.080 Tabela gastos.xlsx\n",
      "06/10/2019  17:44         3.779.179 TareaDB11Tumores-9Algortimos-V4.ipynb\n",
      "04/12/2019  20:27    <DIR>          temp\n",
      "12/11/2017  14:27               283 temp.py\n",
      "19/12/2019  11:48    <DIR>          Timelines\n",
      "28/04/2014  20:25               132 time_code.py\n",
      "19/12/2019  12:13    <DIR>          Tucano\n",
      "20/11/2015  20:38    <DIR>          Ubuntu\n",
      "19/04/2020  15:35    <DIR>          ufscar\n",
      "11/04/2019  15:22               555 Untitled.ipynb\n",
      "20/11/2019  14:52            16.143 Untitled1.ipynb\n",
      "26/11/2019  13:52            16.432 Untitled2.ipynb\n",
      "12/08/2017  20:11    <DIR>          viagem aparecida\n",
      "20/11/2015  20:38    <DIR>          viagem boston\n",
      "20/11/2015  20:38    <DIR>          viagem boston 2\n",
      "02/03/2016  23:12    <DIR>          viagem chicago\n",
      "09/11/2018  15:47    <DIR>          viagem grecia\n",
      "20/11/2015  20:38    <DIR>          viagem hobbit\n",
      "12/01/2019  11:29    <DIR>          viagem ilhabela\n",
      "19/01/2020  15:17    <DIR>          viagem ubatuba\n",
      "07/09/2017  15:35    <DIR>          viagem Washington\n",
      "02/04/2020  21:30        48.278.165 VIDEO-2020-04-02-20-39-25.mp4\n",
      "28/12/2019  14:01    <DIR>          virtualbox\n",
      "21/12/2019  11:43    <DIR>          visualizer\n",
      "20/11/2015  20:38    <DIR>          wekafiles\n",
      "14/03/2017  18:24    <DIR>          Workshop_pos\n",
      "              55 arquivo(s) 12.697.386.131 bytes\n",
      "             140 pasta(s)   189.182.369.792 bytes disponveis\n"
     ]
    }
   ],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('torchtrainer/models/unet/unet.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterateFolderPath(path):\n",
    "    \n",
    "    dirs = path.parts\n",
    "    parPath = Path('.')\n",
    "    for dir in dirs:\n",
    "        parPath = parPath/dir\n",
    "        yield parPath\n",
    "        \n",
    "def createFolder(outFolder):\n",
    "    \n",
    "    for folder in iterateFolderPath(outFolder):\n",
    "        if not folder.exists(): folder.mkdir()\n",
    "\n",
    "createFolder(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models', 'unet', 'unet.py')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.relative_to('torchtrainer').parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torchtrainer/models/unet/unet.py'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.as_posix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
