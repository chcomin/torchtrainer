{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network models (architectures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted models/unet.py to torchtrainer/models/unet.py\n",
      "Converted models/edunet.py to torchtrainer/models/edunet.py\n",
      "Converted models/resunet.py to torchtrainer/models/resunet.py\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Export cells\n",
    "!python notebook2script.py models.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# U-Net simple\n",
    "#export models/unet.py\n",
    "'''U-Net architecture'''\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "class DoubleConvolution(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channel, out_channels, kernel_size=3, p=1):\n",
    "        super(DoubleConvolution, self).__init__()\n",
    "        layers = [\n",
    "            nn.Conv2d(in_channels, middle_channel, kernel_size=kernel_size, padding=p),\n",
    "            nn.BatchNorm2d(middle_channel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(middle_channel, out_channels, kernel_size=kernel_size, padding=p),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        self.dconv = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dconv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, num_channels, num_classes):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        reduce_by = 1\n",
    "\n",
    "        self.l1_ = DoubleConvolution(num_channels, 64//reduce_by, 64//reduce_by)\n",
    "        self.a1_dwn = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.l2_ = DoubleConvolution(64//reduce_by, 128//reduce_by, 128//reduce_by)\n",
    "        self.a2_dwn = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.l3_ = DoubleConvolution(128//reduce_by, 256//reduce_by, 256//reduce_by)\n",
    "        self.a3_dwn = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.l4_ = DoubleConvolution(256//reduce_by, 512//reduce_by, 512//reduce_by)\n",
    "        self.a4_dwn = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.l_mid = DoubleConvolution(512//reduce_by, 1024//reduce_by, 1024//reduce_by)   \n",
    "        \n",
    "        self.a_mid_up = nn.ConvTranspose2d(1024//reduce_by, 512//reduce_by, kernel_size=2, stride=2)\n",
    "        self._l4 = DoubleConvolution(1024//reduce_by, 512//reduce_by, 512//reduce_by)\n",
    "\n",
    "        self.a4_up = nn.ConvTranspose2d(512//reduce_by, 256//reduce_by, kernel_size=2, stride=2)\n",
    "        self._l3 = DoubleConvolution(512//reduce_by, 256//reduce_by, 256//reduce_by)\n",
    "\n",
    "        self.a3_up = nn.ConvTranspose2d(256//reduce_by, 128//reduce_by, kernel_size=2, stride=2)\n",
    "        self._l2 = DoubleConvolution(256//reduce_by, 128//reduce_by, 128//reduce_by)\n",
    "\n",
    "        self.a2_up = nn.ConvTranspose2d(128//reduce_by, 64//reduce_by, kernel_size=2, stride=2)\n",
    "        self._l1 = DoubleConvolution(128//reduce_by, 64//reduce_by, 64//reduce_by)\n",
    "\n",
    "        self.final = nn.Conv2d(64//reduce_by, num_classes, kernel_size=1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def forward(self, x): \n",
    "        \n",
    "        a1_ = self.l1_(x)\n",
    "        a1_dwn = self.a1_dwn(a1_)\n",
    "\n",
    "        a2_ = self.l2_(a1_dwn)\n",
    "        a2_dwn = self.a2_dwn(a2_)\n",
    "\n",
    "        a3_ = self.l3_(a2_dwn)\n",
    "        a3_dwn = self.a3_dwn(a3_)\n",
    "\n",
    "        a4_ = self.l4_(a3_dwn)\n",
    "        # a4_ = F.dropout(a4_, p=0.2)\n",
    "        a4_dwn = self.a4_dwn(a4_)\n",
    "\n",
    "        a_mid = self.l_mid(a4_dwn)                      \n",
    "        \n",
    "        a_mid_up = self.a_mid_up(a_mid)                              \n",
    "        _a4 = self._l4(UNet.match_and_concat(a4_, a_mid_up))      \n",
    "        # _a4 = F.dropout(_a4, p=0.2)\n",
    "\n",
    "        a4_up = self.a4_up(_a4)                                \n",
    "        _a3 = self._l3(UNet.match_and_concat(a3_, a4_up))      \n",
    "\n",
    "        a3_up = self.a3_up(_a3)                                \n",
    "        _a2 = self._l2(UNet.match_and_concat(a2_, a3_up))      \n",
    "        # _a2 = F.dropout(_a2, p=0.2)\n",
    "\n",
    "        a2_up = self.a2_up(_a2)                                \n",
    "        _a1 = self._l1(UNet.match_and_concat(a1_, a2_up))     \n",
    "\n",
    "        final = self.final(_a1)\n",
    "        return F.log_softmax(final, 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def match_and_concat(bypass, upsampled, crop=True):\n",
    "        \n",
    "        if crop:\n",
    "            c_h = (bypass.shape[2] - upsampled.shape[2])\n",
    "            c_w = (bypass.shape[3] - upsampled.shape[3])\n",
    "            if c_h%2==0:\n",
    "                c_hu = c_hd = c_h//2\n",
    "            else:\n",
    "                c_hu = c_h//2\n",
    "                c_hd = c_h//2+1\n",
    "            if c_w%2==0:\n",
    "                c_wl = c_wr = c_w//2\n",
    "            else:\n",
    "                c_wl = c_w//2\n",
    "                c_wr = c_w//2+1\n",
    "                \n",
    "            bypass = F.pad(bypass, (-c_wl, -c_wr, -c_hu, -c_hd))\n",
    "        return torch.cat((upsampled, bypass), 1)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "                nn.init.kaiming_normal_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data.zero_()\n",
    "            elif isinstance(module, nn.BatchNorm2d):\n",
    "                module.weight.data.fill_(1)\n",
    "                module.bias.data.zero_()\n",
    "                    \n",
    "    def get_shapes(self, img_shape):\n",
    "\n",
    "        input_img = torch.zeros(img_shape)[None, None]\n",
    "        input_img = input_img.to(next(model.parameters()).device)\n",
    "        output = self(input_img)\n",
    "        return output[0, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# U-Net encoder decoder\n",
    "#export models/edunet.py\n",
    "'''U-Net architecture'''\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import tensor\n",
    "\n",
    "# For importing in both the notebook and in the .py file\n",
    "try:\n",
    "    import ActivationSampler\n",
    "except ImportError:\n",
    "    from torchtrainer.module_util import ActivationSampler\n",
    "\n",
    "class DoubleConvolution(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channel, out_channels, kernel_size=3, p=1):\n",
    "        super(DoubleConvolution, self).__init__()\n",
    "        layers = [\n",
    "            nn.Conv2d(in_channels, middle_channel, kernel_size=kernel_size, padding=p),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(middle_channel),\n",
    "            nn.Conv2d(middle_channel, out_channels, kernel_size=kernel_size, padding=p),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        ]\n",
    "        self.dconv = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dconv(x)\n",
    "    \n",
    "class ResDoubleConvolution(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channel, out_channels, kernel_size=3, p=1):\n",
    "        super(ResDoubleConvolution, self).__init__()\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        layers = [\n",
    "            nn.Conv2d(in_channels, middle_channel, kernel_size=kernel_size, padding=p, bias=False),\n",
    "            nn.BatchNorm2d(middle_channel),\n",
    "            self.relu,\n",
    "            nn.Conv2d(middle_channel, out_channels, kernel_size=kernel_size, padding=p, bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        ]\n",
    "        self.dconv = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.relu(self.dconv(x) + x)\n",
    "    \n",
    "class Concat(nn.Module):\n",
    "    '''Module for concatenating two activations'''\n",
    "\n",
    "    def __init__(self, concat_dim=1):\n",
    "        super(Concat, self).__init__()    \n",
    "        self.concat_dim = concat_dim\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        # Inputs will be padded if not the same size\n",
    "        \n",
    "        x1, x2 = self.pad_inputs(x1, x2)\n",
    "        return torch.cat((x1, x2), self.concat_dim)\n",
    "    \n",
    "    def pad_inputs(self, x1, x2):\n",
    "        \n",
    "        cd = self.concat_dim\n",
    "        shape_diff = tensor(x2.shape[cd+1:]) - tensor(x1.shape[cd+1:])\n",
    "        pad1 = []\n",
    "        pad2 = []\n",
    "        for sd in shape_diff.flip(0):\n",
    "            sd_abs = abs(sd.item())\n",
    "            if sd%2==0:\n",
    "                pb = pe = sd_abs//2\n",
    "            else:\n",
    "                pb = sd_abs//2\n",
    "                pe = pb + 1\n",
    "                \n",
    "            if sd>=0:\n",
    "                pad1 += [pb, pe]\n",
    "                pad2 += [0, 0]\n",
    "            else:\n",
    "                pad1 += [0, 0]\n",
    "                pad2 += [pb, pe]\n",
    "                \n",
    "        x1 = F.pad(x1, pad1)\n",
    "        x2 = F.pad(x2, pad2)\n",
    "        \n",
    "        return x1, x2\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        s = 'concat_dim={concat_dim}'\n",
    "        return s.format(**self.__dict__)\n",
    "        \n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    '''Encoder part of U-Net'''\n",
    "    \n",
    "    def __init__(self, num_channels, ConvBlock, reduce_by=1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.l1_ = ConvBlock(num_channels, 64//reduce_by, 64//reduce_by)\n",
    "        self.a1_dwn = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.l2_ = ConvBlock(64//reduce_by, 128//reduce_by, 128//reduce_by)\n",
    "        self.a2_dwn = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.l3_ = ConvBlock(128//reduce_by, 256//reduce_by, 256//reduce_by)\n",
    "        self.a3_dwn = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.l4_ = ConvBlock(256//reduce_by, 512//reduce_by, 512//reduce_by)\n",
    "        self.a4_dwn = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.l_mid = ConvBlock(512//reduce_by, 1024//reduce_by, 1024//reduce_by)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.children(): x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EDUNet(nn.Module):\n",
    "    def __init__(self, num_channels, num_classes):\n",
    "        super(EDUNet, self).__init__()\n",
    "\n",
    "        reduce_by = 1            \n",
    "\n",
    "        ConvBlock = DoubleConvolution\n",
    "        self.encoder = Encoder(num_channels, ConvBlock)\n",
    "                  \n",
    "        self.a_mid_up = nn.ConvTranspose2d(1024//reduce_by, 512//reduce_by, kernel_size=2, stride=2)\n",
    "        self.sample_a4_ = ActivationSampler(self.encoder.l4_)\n",
    "        self.concat_a4 = Concat(1)\n",
    "        self._l4 = ConvBlock(1024//reduce_by, 512//reduce_by, 512//reduce_by)\n",
    "\n",
    "        self.a4_up = nn.ConvTranspose2d(512//reduce_by, 256//reduce_by, kernel_size=2, stride=2)\n",
    "        self.sample_a3_ = ActivationSampler(self.encoder.l3_)\n",
    "        self.concat_a3 = Concat(1)\n",
    "        self._l3 = ConvBlock(512//reduce_by, 256//reduce_by, 256//reduce_by)\n",
    "\n",
    "        self.a3_up = nn.ConvTranspose2d(256//reduce_by, 128//reduce_by, kernel_size=2, stride=2)\n",
    "        self.sample_a2_ = ActivationSampler(self.encoder.l2_)\n",
    "        self.concat_a2 = Concat(1)\n",
    "        self._l2 = ConvBlock(256//reduce_by, 128//reduce_by, 128//reduce_by)\n",
    "\n",
    "        self.a2_up = nn.ConvTranspose2d(128//reduce_by, 64//reduce_by, kernel_size=2, stride=2)\n",
    "        self.sample_a1_ = ActivationSampler(self.encoder.l1_)\n",
    "        self.concat_a1 = Concat(1)\n",
    "        self._l1 = ConvBlock(128//reduce_by, 64//reduce_by, 64//reduce_by)\n",
    "\n",
    "        self.final = nn.Conv2d(64//reduce_by, num_classes, kernel_size=1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def forward(self, x):    \n",
    "        a_mid = self.encoder(x)                        \n",
    "        \n",
    "        a_mid_up = self.a_mid_up(a_mid)  \n",
    "        a4_ = self.sample_a4_()\n",
    "        _a4 = self._l4(self.concat_a4(a4_, a_mid_up))      \n",
    "        # _a4 = F.dropout(_a4, p=0.2)\n",
    "\n",
    "        a4_up = self.a4_up(_a4)           \n",
    "        a3_ = self.sample_a3_()\n",
    "        _a3 = self._l3(self.concat_a3(a3_, a4_up))      \n",
    "\n",
    "        a3_up = self.a3_up(_a3)      \n",
    "        a2_ = self.sample_a2_()\n",
    "        _a2 = self._l2(self.concat_a2(a2_, a3_up))      \n",
    "        # _a2 = F.dropout(_a2, p=0.2)\n",
    "\n",
    "        a2_up = self.a2_up(_a2)  \n",
    "        a1_ = self.sample_a1_()\n",
    "        _a1 = self._l1(self.concat_a1(a1_, a2_up))     \n",
    "\n",
    "        final = self.final(_a1)\n",
    "        return F.log_softmax(final, 1)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "                nn.init.kaiming_normal_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data.zero_()\n",
    "            elif isinstance(module, nn.BatchNorm2d):\n",
    "                module.weight.data.fill_(1)\n",
    "                module.bias.data.zero_()\n",
    "                    \n",
    "    def get_shapes(self, img_shape):\n",
    "\n",
    "        input_img = torch.zeros(img_shape)[None, None]\n",
    "        input_img = input_img.to(next(model.parameters()).device)\n",
    "        output = self(input_img)\n",
    "        return output[0, 0].shape\n",
    "    \n",
    "class ActivationSampler(nn.Module):\n",
    "    '''Generates a hook for sampling a layer activation'''\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        super(ActivationSampler, self).__init__()\n",
    "        self.model_name = model.__class__.__name__\n",
    "        self.activation = None\n",
    "        model.register_forward_hook(self.get_hook())\n",
    "        \n",
    "    def forward(self, x=None):\n",
    "        return self.activation\n",
    "    \n",
    "    def get_hook(self):\n",
    "        def hook(model, input, output):\n",
    "            self.activation = output\n",
    "        return hook\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return f'{self.model_name}'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# U-Net encoder decoder ResNet\n",
    "#export models/resunet.py\n",
    "'''U-Net architecture with residual blocks'''\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import tensor\n",
    "\n",
    "# For importing in both the notebook and in the .py file\n",
    "try:\n",
    "    import ActivationSampler\n",
    "except ImportError:\n",
    "    from torchtrainer.module_util import ActivationSampler\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "    \n",
    "class ResBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, norm_layer=None):\n",
    "        super(ResBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self.stride = stride\n",
    "          \n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        \n",
    "        if (inplanes!=planes) or (stride>1):\n",
    "            # If in and out planes are different, we also need to change the planes of the input\n",
    "            # If stride is not 1, we need to change the size of the input\n",
    "            reshape_input = nn.Sequential(\n",
    "                                    conv1x1(inplanes, planes, stride),\n",
    "                                    norm_layer(planes),\n",
    "                            )\n",
    "            self.reshape_input = reshape_input\n",
    "        else:\n",
    "            self.reshape_input = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.reshape_input is not None:\n",
    "            identity = self.reshape_input(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "class Concat(nn.Module):\n",
    "    '''Module for concatenating two activations'''\n",
    "\n",
    "    def __init__(self, concat_dim=1):\n",
    "        super(Concat, self).__init__()    \n",
    "        self.concat_dim = concat_dim\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        # Inputs will be padded if not the same size\n",
    "        \n",
    "        x1, x2 = self.pad_inputs(x1, x2)\n",
    "        return torch.cat((x1, x2), self.concat_dim)\n",
    "    \n",
    "    def pad_inputs(self, x1, x2):\n",
    "        \n",
    "        cd = self.concat_dim\n",
    "        shape_diff = tensor(x2.shape[cd+1:]) - tensor(x1.shape[cd+1:])\n",
    "        pad1 = []\n",
    "        pad2 = []\n",
    "        for sd in shape_diff.flip(0):\n",
    "            sd_abs = abs(sd.item())\n",
    "            if sd%2==0:\n",
    "                pb = pe = sd_abs//2\n",
    "            else:\n",
    "                pb = sd_abs//2\n",
    "                pe = pb + 1\n",
    "                \n",
    "            if sd>=0:\n",
    "                pad1 += [pb, pe]\n",
    "                pad2 += [0, 0]\n",
    "            else:\n",
    "                pad1 += [0, 0]\n",
    "                pad2 += [pb, pe]\n",
    "                \n",
    "        x1 = F.pad(x1, pad1)\n",
    "        x2 = F.pad(x2, pad2)\n",
    "        \n",
    "        return x1, x2\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        s = 'concat_dim={concat_dim}'\n",
    "        return s.format(**self.__dict__)\n",
    "        \n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    '''Encoder part of U-Net'''\n",
    "    \n",
    "    def __init__(self, num_channels, reduce_by=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        #num_planes = 64\n",
    "        self.conv1 = nn.Conv2d(num_channels, 64, kernel_size=7, stride=1, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.resblock1 = ResBlock(64, 64, stride=1)\n",
    "        self.resblock2 = ResBlock(64, 128, stride=2)\n",
    "        self.resblock3 = ResBlock(128, 256, stride=2)\n",
    "        self.resblock4 = ResBlock(256, 512, stride=2)\n",
    "        self.resblock_mid = ResBlock(512, 1024, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.children(): x = layer(x)\n",
    "        return x\n",
    "    \n",
    "class ResUNet(nn.Module):\n",
    "    # TODO: fix output size being different than input\n",
    "    \n",
    "    def __init__(self, num_channels, num_classes):\n",
    "        super(ResUNet, self).__init__()\n",
    "         \n",
    "        self.encoder = Encoder(num_channels)\n",
    "                  \n",
    "        self.a_mid_up = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.blur_mid_up = Blur()\n",
    "        self.sample_a4_ = ActivationSampler(self.encoder.resblock4)\n",
    "        self.concat_a4 = Concat(1)\n",
    "        self._l4 = ResBlock(1024, 512, stride=1)\n",
    "\n",
    "        self.a4_up = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.blur_a4_up = Blur()\n",
    "        self.sample_a3_ = ActivationSampler(self.encoder.resblock3)\n",
    "        self.concat_a3 = Concat(1)\n",
    "        self._l3 = ResBlock(512, 256, stride=1)\n",
    "\n",
    "        self.a3_up = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.blur_a3_up = Blur()\n",
    "        self.sample_a2_ = ActivationSampler(self.encoder.resblock2)\n",
    "        self.concat_a2 = Concat(1)\n",
    "        self._l2 = ResBlock(256, 128, stride=1)\n",
    "\n",
    "        self.a2_up = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.blur_a2_up = Blur()\n",
    "        self.sample_a1_ = ActivationSampler(self.encoder.resblock1)\n",
    "        self.concat_a1 = Concat(1)\n",
    "        self._l1 = ResBlock(128, 64, stride=1)\n",
    "\n",
    "        self.final = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def forward(self, x):    \n",
    "        a_mid = self.encoder(x)                        \n",
    "        \n",
    "        a_mid_up = self.a_mid_up(a_mid)  \n",
    "        a_mid_up = self.blur_mid_up(a_mid_up)\n",
    "        a4_ = self.sample_a4_()\n",
    "        _a4 = self._l4(self.concat_a4(a4_, a_mid_up))      \n",
    "        # _a4 = F.dropout(_a4, p=0.2)\n",
    "\n",
    "        a4_up = self.a4_up(_a4)   \n",
    "        a4_up = self.blur_a4_up(a4_up)\n",
    "        a3_ = self.sample_a3_()\n",
    "        _a3 = self._l3(self.concat_a3(a3_, a4_up))      \n",
    "\n",
    "        a3_up = self.a3_up(_a3)      \n",
    "        a3_up = self.blur_a3_up(a3_up)\n",
    "        a2_ = self.sample_a2_()\n",
    "        _a2 = self._l2(self.concat_a2(a2_, a3_up))      \n",
    "        # _a2 = F.dropout(_a2, p=0.2)\n",
    "\n",
    "        a2_up = self.a2_up(_a2)  \n",
    "        a2_up = self.blur_a2_up(a2_up)\n",
    "        a1_ = self.sample_a1_()\n",
    "        _a1 = self._l1(self.concat_a1(a1_, a2_up))     \n",
    "\n",
    "        final = self.final(_a1)\n",
    "        return F.log_softmax(final, 1)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "                nn.init.kaiming_normal_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data.zero_()\n",
    "            elif isinstance(module, nn.BatchNorm2d):\n",
    "                module.weight.data.fill_(1)\n",
    "                module.bias.data.zero_()\n",
    "                    \n",
    "    def get_shapes(self, img_shape):\n",
    "\n",
    "        input_img = torch.zeros(img_shape)[None, None]\n",
    "        input_img = input_img.to(next(model.parameters()).device)\n",
    "        output = self(input_img)\n",
    "        return output[0, 0].shape\n",
    "\n",
    "class Blur(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Blur, self).__init__()\n",
    "        \n",
    "        self.pad = nn.ReplicationPad2d((0,1,0,1))\n",
    "        self.blur = nn.AvgPool2d(2, stride=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.blur(self.pad(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Dynamic UNet from Fastai (not working)\n",
    "import fastai\n",
    "import fastai.vision as fai_vision\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "def DUNet(n_classes, img_size):\n",
    "    \n",
    "    resnet = fastai.vision.models.resnet34(True)\n",
    "    del resnet.avgpool\n",
    "    del resnet.fc\n",
    "    model = torch.nn.Sequential(*list(resnet.children()))\n",
    "    \n",
    "    return fai_vision.models.unet.DynamicUnet(model, n_classes=n_classes, img_size=img_size)\n",
    "\n",
    "#resnet34 = torchvision.models.resnet34(False)\n",
    "#unetai = DUNet(2, (1000,1000))\n",
    "edunet = EDUNet(3, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb = torch.randn((2, 3, 572, 572)).to('cuda')\n",
    "model = ResUNet(3, 2)\n",
    "model.to('cuda')\n",
    "\n",
    "a1_ = ActivationSampler(model.encoder.resblock1)\n",
    "a2_ = ActivationSampler(model.encoder.resblock2)\n",
    "a3_ = ActivationSampler(model.encoder.resblock3)\n",
    "a4_ = ActivationSampler(model.encoder.resblock4)\n",
    "a_mid = ActivationSampler(model.encoder.resblock_mid)\n",
    "a_mid_up = ActivationSampler(model.a_mid_up)\n",
    "_a4 = ActivationSampler(model._l4)\n",
    "_a3 = ActivationSampler(model._l3)\n",
    "_a2 = ActivationSampler(model._l2)\n",
    "_a1 = ActivationSampler(model._l1)\n",
    "\n",
    "pred = model(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 572, 572])\n",
      "torch.Size([2, 128, 286, 286])\n",
      "torch.Size([2, 256, 143, 143])\n",
      "torch.Size([2, 512, 72, 72])\n",
      "torch.Size([2, 1024, 36, 36])\n",
      "torch.Size([2, 512, 72, 72])\n",
      "torch.Size([2, 512, 72, 72])\n",
      "torch.Size([2, 256, 144, 144])\n",
      "torch.Size([2, 128, 288, 288])\n",
      "torch.Size([2, 64, 576, 576])\n"
     ]
    }
   ],
   "source": [
    "print(a1_().shape)\n",
    "print(a2_().shape)\n",
    "print(a3_().shape)\n",
    "print(a4_().shape)\n",
    "print(a_mid().shape)\n",
    "print(a_mid_up().shape)\n",
    "print(_a4().shape)\n",
    "print(_a3().shape)\n",
    "print(_a2().shape)\n",
    "print(_a1().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DynamicUnet(\n",
       "  (layers): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (4): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (4): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (5): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    )\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): UnetBlock(\n",
       "      (shuf): PixelShuffle_ICNR(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (shuf): PixelShuffle(upscale_factor=2)\n",
       "        (pad): ReplicationPad2d((1, 0, 1, 0))\n",
       "        (blur): AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (5): UnetBlock(\n",
       "      (shuf): PixelShuffle_ICNR(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(768, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (shuf): PixelShuffle(upscale_factor=2)\n",
       "        (pad): ReplicationPad2d((1, 0, 1, 0))\n",
       "        (blur): AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (6): UnetBlock(\n",
       "      (shuf): PixelShuffle_ICNR(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (shuf): PixelShuffle(upscale_factor=2)\n",
       "        (pad): ReplicationPad2d((1, 0, 1, 0))\n",
       "        (blur): AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (7): UnetBlock(\n",
       "      (shuf): PixelShuffle_ICNR(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(448, 896, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (shuf): PixelShuffle(upscale_factor=2)\n",
       "        (pad): ReplicationPad2d((1, 0, 1, 0))\n",
       "        (blur): AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (8): UnetBlock(\n",
       "      (shuf): PixelShuffle_ICNR(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(288, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (shuf): PixelShuffle(upscale_factor=2)\n",
       "        (pad): ReplicationPad2d((1, 0, 1, 0))\n",
       "        (blur): AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(208, 104, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): BatchNorm2d(104, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(104, 104, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): BatchNorm2d(104, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (9): PixelShuffle_ICNR(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(104, 416, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (shuf): PixelShuffle(upscale_factor=2)\n",
       "      (pad): ReplicationPad2d((1, 0, 1, 0))\n",
       "      (blur): AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (10): MergeLayer()\n",
       "    (11): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(107, 107, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): BatchNorm2d(107, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2d(107, 107, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): BatchNorm2d(107, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): MergeLayer()\n",
       "      )\n",
       "    )\n",
       "    (12): Sequential(\n",
       "      (0): Conv2d(107, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unetai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 500, 500]           1,792\n",
      "              ReLU-2         [-1, 64, 500, 500]               0\n",
      "       BatchNorm2d-3         [-1, 64, 500, 500]             128\n",
      "            Conv2d-4         [-1, 64, 500, 500]          36,928\n",
      "              ReLU-5         [-1, 64, 500, 500]               0\n",
      "       BatchNorm2d-6         [-1, 64, 500, 500]             128\n",
      " DoubleConvolution-7         [-1, 64, 500, 500]               0\n",
      "         MaxPool2d-8         [-1, 64, 250, 250]               0\n",
      "            Conv2d-9        [-1, 128, 250, 250]          73,856\n",
      "             ReLU-10        [-1, 128, 250, 250]               0\n",
      "      BatchNorm2d-11        [-1, 128, 250, 250]             256\n",
      "           Conv2d-12        [-1, 128, 250, 250]         147,584\n",
      "             ReLU-13        [-1, 128, 250, 250]               0\n",
      "      BatchNorm2d-14        [-1, 128, 250, 250]             256\n",
      "DoubleConvolution-15        [-1, 128, 250, 250]               0\n",
      "        MaxPool2d-16        [-1, 128, 125, 125]               0\n",
      "           Conv2d-17        [-1, 256, 125, 125]         295,168\n",
      "             ReLU-18        [-1, 256, 125, 125]               0\n",
      "      BatchNorm2d-19        [-1, 256, 125, 125]             512\n",
      "           Conv2d-20        [-1, 256, 125, 125]         590,080\n",
      "             ReLU-21        [-1, 256, 125, 125]               0\n",
      "      BatchNorm2d-22        [-1, 256, 125, 125]             512\n",
      "DoubleConvolution-23        [-1, 256, 125, 125]               0\n",
      "        MaxPool2d-24          [-1, 256, 62, 62]               0\n",
      "           Conv2d-25          [-1, 512, 62, 62]       1,180,160\n",
      "             ReLU-26          [-1, 512, 62, 62]               0\n",
      "      BatchNorm2d-27          [-1, 512, 62, 62]           1,024\n",
      "           Conv2d-28          [-1, 512, 62, 62]       2,359,808\n",
      "             ReLU-29          [-1, 512, 62, 62]               0\n",
      "      BatchNorm2d-30          [-1, 512, 62, 62]           1,024\n",
      "DoubleConvolution-31          [-1, 512, 62, 62]               0\n",
      "        MaxPool2d-32          [-1, 512, 31, 31]               0\n",
      "           Conv2d-33         [-1, 1024, 31, 31]       4,719,616\n",
      "             ReLU-34         [-1, 1024, 31, 31]               0\n",
      "      BatchNorm2d-35         [-1, 1024, 31, 31]           2,048\n",
      "           Conv2d-36         [-1, 1024, 31, 31]       9,438,208\n",
      "             ReLU-37         [-1, 1024, 31, 31]               0\n",
      "      BatchNorm2d-38         [-1, 1024, 31, 31]           2,048\n",
      "DoubleConvolution-39         [-1, 1024, 31, 31]               0\n",
      "          Encoder-40         [-1, 1024, 31, 31]               0\n",
      "  ConvTranspose2d-41          [-1, 512, 62, 62]       2,097,664\n",
      "ActivationSampler-42          [-1, 512, 62, 62]               0\n",
      "           Concat-43         [-1, 1024, 62, 62]               0\n",
      "           Conv2d-44          [-1, 512, 62, 62]       4,719,104\n",
      "             ReLU-45          [-1, 512, 62, 62]               0\n",
      "      BatchNorm2d-46          [-1, 512, 62, 62]           1,024\n",
      "           Conv2d-47          [-1, 512, 62, 62]       2,359,808\n",
      "             ReLU-48          [-1, 512, 62, 62]               0\n",
      "      BatchNorm2d-49          [-1, 512, 62, 62]           1,024\n",
      "DoubleConvolution-50          [-1, 512, 62, 62]               0\n",
      "  ConvTranspose2d-51        [-1, 256, 124, 124]         524,544\n",
      "ActivationSampler-52        [-1, 256, 125, 125]               0\n",
      "           Concat-53        [-1, 512, 125, 125]               0\n",
      "           Conv2d-54        [-1, 256, 125, 125]       1,179,904\n",
      "             ReLU-55        [-1, 256, 125, 125]               0\n",
      "      BatchNorm2d-56        [-1, 256, 125, 125]             512\n",
      "           Conv2d-57        [-1, 256, 125, 125]         590,080\n",
      "             ReLU-58        [-1, 256, 125, 125]               0\n",
      "      BatchNorm2d-59        [-1, 256, 125, 125]             512\n",
      "DoubleConvolution-60        [-1, 256, 125, 125]               0\n",
      "  ConvTranspose2d-61        [-1, 128, 250, 250]         131,200\n",
      "ActivationSampler-62        [-1, 128, 250, 250]               0\n",
      "           Concat-63        [-1, 256, 250, 250]               0\n",
      "           Conv2d-64        [-1, 128, 250, 250]         295,040\n",
      "             ReLU-65        [-1, 128, 250, 250]               0\n",
      "      BatchNorm2d-66        [-1, 128, 250, 250]             256\n",
      "           Conv2d-67        [-1, 128, 250, 250]         147,584\n",
      "             ReLU-68        [-1, 128, 250, 250]               0\n",
      "      BatchNorm2d-69        [-1, 128, 250, 250]             256\n",
      "DoubleConvolution-70        [-1, 128, 250, 250]               0\n",
      "  ConvTranspose2d-71         [-1, 64, 500, 500]          32,832\n",
      "ActivationSampler-72         [-1, 64, 500, 500]               0\n",
      "           Concat-73        [-1, 128, 500, 500]               0\n",
      "           Conv2d-74         [-1, 64, 500, 500]          73,792\n",
      "             ReLU-75         [-1, 64, 500, 500]               0\n",
      "      BatchNorm2d-76         [-1, 64, 500, 500]             128\n",
      "           Conv2d-77         [-1, 64, 500, 500]          36,928\n",
      "             ReLU-78         [-1, 64, 500, 500]               0\n",
      "      BatchNorm2d-79         [-1, 64, 500, 500]             128\n",
      "DoubleConvolution-80         [-1, 64, 500, 500]               0\n",
      "           Conv2d-81          [-1, 2, 500, 500]             130\n",
      "           EDUNet-82          [-1, 2, 500, 500]               0\n",
      "================================================================\n",
      "Total params: 31,043,586\n",
      "Trainable params: 31,043,586\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 2.86\n",
      "Forward/backward pass size (MB): 4239.74\n",
      "Params size (MB): 118.42\n",
      "Estimated Total Size (MB): 4361.02\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(31043586), tensor(31043586))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorchlb.torchsummary import summary\n",
    "edunet.to('cuda')\n",
    "summary(edunet, input_size=(3, 500, 500))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
