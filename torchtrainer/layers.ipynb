{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted models/layers.py to torchtrainer/models/layers.py\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Export cells\n",
    "!python notebook2script.py layers.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-hair convolution filters\n",
    "#export models/layers.py\n",
    "from collections.abc import Iterable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def ntuple(x, n):\n",
    "    '''Verify if x is iterable. If not, create tuple containing x repeated n times'''\n",
    "    \n",
    "    if isinstance(x, Iterable):\n",
    "        return x\n",
    "    return tuple([x]*n)\n",
    "\n",
    "class Conv2dCH(nn.Module):\n",
    "    \"\"\"Create 2D cross-hair convolution filter. Parameters are the same as torch.nn.Conv2d, with the exception\n",
    "    that padding must be larger than or equal to (kernel_size-1)//2 (otherwise the filter would need negative padding\n",
    "    to properly work) and dilation is not supported. Also, if padding is not provided it will be equal to (kernel_size-1)//2.\n",
    "    That is, by default the result of the convolution has the same shape as the input tensor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_channels : int\n",
    "        Number of input channels. \n",
    "    out_channels : int\n",
    "        Number of output channels. \n",
    "    kernel_size : int or tuple\n",
    "        Size of the kernel. Even sizes are not supported. \n",
    "    stride : int or tuple\n",
    "        Stride of the convolution.\n",
    "    padding : int or tuple\n",
    "        Padding of the input. Must be larger than or equal to (kernel_size-1)//2.\n",
    "    groups : int\n",
    "        Controls the connections between inputs and outputs. \n",
    "    bias : bool\n",
    "        If True, adds a learnable bias to the output.\n",
    "    padding_mode : string\n",
    "        Padding mode to use.\n",
    "    \"\"\"    \n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=None, groups=1, bias=True, \n",
    "                 padding_mode='zeros'):\n",
    "        super(Conv2dCH, self).__init__()\n",
    "        \n",
    "        kernel_size = ntuple(kernel_size, 2)\n",
    "        stride = ntuple(stride, 2)\n",
    "        padding = ntuple(padding, 2)\n",
    "        \n",
    "        padding = list(padding)\n",
    "        if padding[0] is None:\n",
    "            padding[0] = (kernel_size[0]-1)//2\n",
    "        if padding[1] is None:\n",
    "            padding[1] = (kernel_size[1]-1)//2\n",
    "        \n",
    "        if padding[0]<(kernel_size[0]-1)//2:\n",
    "            raise ValueError(\"Padding must be padding[0]>=(kernel_size[0]-1)//2\")\n",
    "        if padding[1]<(kernel_size[1]-1)//2:\n",
    "            raise ValueError(\"Padding must be padding[1]>=(kernel_size[1]-1)//2\")\n",
    "            \n",
    "        pad_conv1d_v_h = padding[1]-(kernel_size[1]-1)//2\n",
    "        pad_conv1d_h_v = padding[0]-(kernel_size[0]-1)//2\n",
    "        self.conv1d_v = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, \n",
    "                                 kernel_size=(kernel_size[0], 1), stride=stride, padding=(padding[0], pad_conv1d_v_h), \n",
    "                                 groups=groups, bias=bias, padding_mode=padding_mode) \n",
    "        self.conv1d_h = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, \n",
    "                                 kernel_size=(1, kernel_size[1]), stride=stride, padding=(pad_conv1d_h_v, padding[1]), \n",
    "                                 groups=groups, bias=bias, padding_mode=padding_mode)    \n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv1d_v(x) + self.conv1d_h(x)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \n",
    "        self.conv1d_v.reset_parameters()\n",
    "        self.conv1d_h.reset_parameters()\n",
    "\n",
    "        '''for module in self.modules():\n",
    "            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "                nn.init.kaiming_normal_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data.zero_()'''\n",
    "        \n",
    "class Conv3dCH(nn.Module):\n",
    "    \"\"\"Create 3D cross-hair convolution filter. Parameters are the same as torch.nn.Conv3d, with the exception\n",
    "    that padding must be larger than or equal to (kernel_size-1)//2 (otherwise the filter would need negative padding\n",
    "    to properly work) and dilation is not supported. Also, if padding is not provided it will be equal to (kernel_size-1)//2.\n",
    "    That is, by default the result of the convolution has the same shape as the input tensor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_channels : int\n",
    "        Number of input channels. \n",
    "    out_channels : int\n",
    "        Number of output channels. \n",
    "    kernel_size : int or tuple\n",
    "        Size of the kernel. Even sizes are not supported. \n",
    "    stride : int or tuple\n",
    "        Stride of the convolution.\n",
    "    padding : int or tuple\n",
    "        Padding of the input. Must be larger than or equal to (kernel_size-1)//2.\n",
    "    groups : int\n",
    "        Controls the connections between inputs and outputs. \n",
    "    bias : bool\n",
    "        If True, adds a learnable bias to the output.\n",
    "    padding_mode : string\n",
    "        Padding mode to use.\n",
    "    \"\"\"    \n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=None, groups=1, bias=True, \n",
    "                 padding_mode='zeros'):\n",
    "        super(Conv3dCH, self).__init__()\n",
    "        \n",
    "        kernel_size = ntuple(kernel_size, 3)\n",
    "        stride = ntuple(stride, 3)\n",
    "        padding = ntuple(padding, 3)\n",
    "        \n",
    "        padding = list(padding)\n",
    "        if padding[0] is None:\n",
    "            padding[0] = (kernel_size[0]-1)//2\n",
    "        if padding[1] is None:\n",
    "            padding[1] = (kernel_size[1]-1)//2\n",
    "        if padding[2] is None:\n",
    "            padding[2] = (kernel_size[2]-1)//2\n",
    "        \n",
    "        if padding[0]<(kernel_size[0]-1)//2:\n",
    "            raise ValueError(\"Padding must be padding[0]>=(kernel_size[0]-1)//2\")\n",
    "        if padding[1]<(kernel_size[1]-1)//2:\n",
    "            raise ValueError(\"Padding must be padding[1]>=(kernel_size[1]-1)//2\")\n",
    "        if padding[2]<(kernel_size[2]-1)//2:\n",
    "            raise ValueError(\"Padding must be padding[2]>=(kernel_size[2]-1)//2\")\n",
    "            \n",
    "        pad_conv2d_p_v = pad_conv2d_h_v = padding[1]-(kernel_size[1]-1)//2\n",
    "        pad_conv2d_p_h = pad_conv2d_v_h = padding[2]-(kernel_size[2]-1)//2\n",
    "        pad_conv2d_v_p = pad_conv2d_h_p = padding[0]-(kernel_size[0]-1)//2\n",
    "\n",
    "        self.conv2d_p = nn.Conv3d(in_channels=in_channels, out_channels=out_channels, \n",
    "                                 kernel_size=(kernel_size[0], 1, 1), stride=stride, \n",
    "                                 padding=(padding[0], pad_conv2d_p_v, pad_conv2d_p_h), \n",
    "                                 groups=groups, bias=bias, padding_mode=padding_mode) \n",
    "        self.conv2d_v = nn.Conv3d(in_channels=in_channels, out_channels=out_channels, \n",
    "                                 kernel_size=(1, kernel_size[1], 1), stride=stride, \n",
    "                                 padding=(pad_conv2d_v_p, padding[1], pad_conv2d_v_h), \n",
    "                                 groups=groups, bias=bias, padding_mode=padding_mode) \n",
    "        self.conv2d_h = nn.Conv3d(in_channels=in_channels, out_channels=out_channels, \n",
    "                                 kernel_size=(1, 1, kernel_size[2]), stride=stride, \n",
    "                                 padding=(pad_conv2d_h_p, pad_conv2d_h_v, padding[2]), \n",
    "                                 groups=groups, bias=bias, padding_mode=padding_mode) \n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv2d_p(x) + self.conv2d_v(x) + self.conv2d_h(x)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \n",
    "        self.conv2d_p.reset_parameters()\n",
    "        self.conv2d_v.reset_parameters()\n",
    "        self.conv2d_h.reset_parameters()\n",
    "\n",
    "        '''for module in self.modules():\n",
    "            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "                nn.init.kaiming_normal_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data.zero_()'''   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conv2dCH(1, 2, (3,3), stride=1, bias=False)(img[None, None])   \n",
    "inp = torch.rand(1, 1, 5, 5, 5)\n",
    "res = Conv3dCH(1, 1, (3, 3, 3), stride=1, bias=False)(inp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class TestModule(nn.Module):\n",
    "\n",
    "    def __init__(self, layer, nlayers, channels, int_channels=16, kernel_size=3):\n",
    "        super(TestModule, self).__init__()\n",
    "        \n",
    "        kernel_size = ntuple(3, 3)\n",
    "        \n",
    "        layers = [layer(channels, int_channels, kernel_size)]\n",
    "        for i in range(nlayers):\n",
    "            layers.append(layer(int_channels, int_channels, kernel_size))\n",
    "        layers.append(layer(int_channels, 1, kernel_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        for layer in self.children():\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "def time_model(model, num_iterations=10, bs=8, channels=32, shape=None):\n",
    "    \n",
    "    if shape is None:\n",
    "        shape = (50, 50)\n",
    "        \n",
    "    model.cuda()\n",
    "\n",
    "    tt = 0\n",
    "    for i in range(num_iterations):\n",
    "        inp = torch.rand(bs, channels, *shape).cuda()\n",
    "        ti = time.time()\n",
    "        res = model(inp)\n",
    "        tf = time.time()\n",
    "        tt += tf-ti\n",
    "    tm = tt/num_iterations\n",
    "    \n",
    "    model.cpu()\n",
    "\n",
    "    return tm\n",
    "\n",
    "\n",
    "conv3d = nn.Conv3d(3, 32, (7, 7, 7))\n",
    "conv3dch = Conv3dCH(3, 32, (7, 7, 7))\n",
    "tm = TestModule(nn.Conv3d, nlayers=18, channels=1, int_channels=32)\n",
    "\n",
    "res = time_model(tm, num_iterations=10, bs=2, channels=1, shape=(70, 70, 70))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0020944833755493163"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestModule()"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
