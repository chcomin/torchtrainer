{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted module_util.py to torchtrainer/module_util.py\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Export cells\n",
    "!python notebook2script.py module_util.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Pytorch module functions\n",
    "#export module_util.py\n",
    "'''Utility functions and classes for working with Pytorch modules'''\n",
    "\n",
    "from torch import nn\n",
    "from functools import partial\n",
    "\n",
    "class ActivationSampler(nn.Module):\n",
    "    '''Generates a hook for sampling a layer activation. Can be used as\n",
    "    \n",
    "    sampler = ActivationSampler(layer_in_model)\n",
    "    output = model(input)\n",
    "    layer_activation = sampler()\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        super(ActivationSampler, self).__init__()\n",
    "        self.model_name = model.__class__.__name__\n",
    "        self.activation = None\n",
    "        model.register_forward_hook(self.get_hook())\n",
    "        \n",
    "    def forward(self, x=None):\n",
    "        return self.activation\n",
    "    \n",
    "    def get_hook(self):\n",
    "        def hook(model, input, output):\n",
    "            self.activation = output\n",
    "        return hook\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return f'{self.model_name}'\n",
    "\n",
    "class Lambda(nn.Module):\n",
    "    '''Transforms function into a module'''\n",
    "    \n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x): return self.func(x)\n",
    "    \n",
    "class Hooks:\n",
    "    '''Hooks for storing information about layers.\n",
    "    \n",
    "    The attribute `storage` will contain the layers information. It is a dict\n",
    "    having layer names as keys and respective values generated by `func`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    module : torch.nn\n",
    "        The module containing the layers. Only used for getting layer names\n",
    "    layers : list\n",
    "        List of torch.nn modules for storing the activations\n",
    "    func : function\n",
    "        Function to be registered as a hook. Must have signature func(storage, module, input, output) for\n",
    "        forward hooks and ?? for backward hooks. `storage` is a dictionary used for storing layer information.    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, module, layers, func, is_forward=True): \n",
    "        \n",
    "        self.hooks = []\n",
    "        \n",
    "        storage = {}         # For storing information, each layer will be a key here\n",
    "        layers_dict = {}     # Dict of layer names and actual layers\n",
    "        # Obtain layer names for hashing. Is there a better way?\n",
    "        for layer_name, layer in module.named_modules():\n",
    "            if True in [True for l in layers if layer is l]:\n",
    "                layers_dict[layer_name] = layer\n",
    "                storage[layer_name] = {}\n",
    "        \n",
    "        self.layers_dict = layers_dict\n",
    "        self.storage = storage\n",
    "        \n",
    "        if is_forward:\n",
    "            self._register_forward_hooks(func)\n",
    "        else:\n",
    "            self._register_backward_hooks(func)\n",
    "    \n",
    "    def __del__(self): self.remove_hooks()\n",
    "    \n",
    "    def _register_forward_hooks(self, func):\n",
    "        '''Register one hook for each layer.'''\n",
    "            \n",
    "        for layer_name, layer in self.layers_dict.items():\n",
    "            hook_func = self._generate_hook(func, self.storage[layer_name])\n",
    "            self.hooks.append(layer.register_forward_hook(hook_func))\n",
    "        \n",
    "    def _register_backward_hooks(self, func):\n",
    "        '''Register one hook for each layer.'''\n",
    "            \n",
    "        for layer_name, layer in self.layers_dict.items():\n",
    "            hook_func = self._generate_hook(func, self.storage[layer_name])\n",
    "            self.hooks.append(layer.register_backward_hook(hook_func))\n",
    "        \n",
    "    def _generate_hook(self, func, storage):\n",
    "        '''Generate function to be used in module.register_forward_hook and module.register_backward_hook, fixing\n",
    "        as a first argument to the function an empty dictionary.'''\n",
    "        \n",
    "        return partial(func, storage)\n",
    "        \n",
    "    def to_cpu(self):\n",
    "        pass\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        '''Remove hooks from the network.'''\n",
    "        \n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "            \n",
    "def _calculate_stats(storage, model, input, output, store_act=True, store_weights=False):\n",
    "    \n",
    "    if store_act:\n",
    "        if 'activation' not in storage:\n",
    "            storage['activation'] = {}\n",
    "        if 'mean' not in storage['activation']:\n",
    "            storage['activation']['mean'] = []\n",
    "        if 'std' not in storage['activation']:\n",
    "            storage['activation']['std'] = []\n",
    "        if 'hist' not in storage['activation']:\n",
    "            storage['activation']['hist'] = []\n",
    "        \n",
    "        activation = output.detach()\n",
    "        storage['activation']['mean'].append(activation.mean().item())\n",
    "        storage['activation']['std'].append(activation.std().item())\n",
    "        storage['activation']['hist'].append(activation.cpu().histc(100,-10,10)) #histc isn't implemented on the GPU\n",
    "                                            \n",
    "    if store_weights:\n",
    "        if 'weights' not in storage:\n",
    "            storage['weights'] = {}\n",
    "        if 'mean' not in storage['weights']:\n",
    "            storage['weights']['mean'] = []\n",
    "        if 'std' not in storage['weights']:\n",
    "            storage['weights']['std'] = []\n",
    "        \n",
    "        try:\n",
    "            weight = model.weight\n",
    "        except Exception:\n",
    "            raise AttributeError('Model does not have `weight` attribute')\n",
    "        else:\n",
    "            weight = weight.detach()\n",
    "            storage['weights']['mean'].append(weight.mean().item())\n",
    "            storage['weights']['std'].append(weight.std().item())\n",
    "            storage['weights']['hist'].append(weight.cpu().histc(100,-10,10)) #histc isn't implemented on the GPU\n",
    "            \n",
    "def calculate_stats(store_act=True, store_weights=False):\n",
    "\n",
    "    return partial(_calculate_stats, store_act=store_act, store_weights=store_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for splitting a model into different groups, which can be frozen or receive distinct learning rates\n",
    "#export module_util.py\n",
    "\n",
    "bn_types = (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)\n",
    "\n",
    "def split_modules(model, modules_to_split):\n",
    "    '''Split `model` layers into different groups. Useful for freezing part of the model\n",
    "    or using different learning rates.'''\n",
    "    \n",
    "    module_groups = [[]]\n",
    "    for module in model.modules():\n",
    "        if module in modules_to_split:\n",
    "            module_groups.append([])\n",
    "        module_groups[-1].append(module)\n",
    "    return module_groups\n",
    "\n",
    "def define_opt_params(module_groups, lr=None, wd=None, debug=False):\n",
    "    '''Define distinct learning rate and weight decay for parameters belonging\n",
    "    to groupd modules in `module_groups`. '''\n",
    "    \n",
    "    num_groups = len(module_groups)\n",
    "    if isinstance(lr, int): lr = [lr]*num_groups\n",
    "    if isinstance(wd, int): wd = [wd]*num_groups\n",
    "    \n",
    "    opt_params = []\n",
    "    for idx, group in enumerate(module_groups):\n",
    "        group_params = {'params':[]}\n",
    "        if lr is not None: group_params['lr'] = lr[idx]\n",
    "        if wd is not None: group_params['wd'] = wd[idx]\n",
    "        for module in group:\n",
    "            pars = module.parameters(recurse=False)\n",
    "            if debug: print(module.__class__)\n",
    "            pars = list(filter(lambda p: p.requires_grad, pars))\n",
    "            if len(pars)>0:\n",
    "                group_params['params'] += pars\n",
    "                if debug:\n",
    "                    for p in pars:\n",
    "                        print(p.shape)\n",
    "        opt_params.append(group_params)\n",
    "    return opt_params\n",
    "\n",
    "def groups_requires_grad(module_groups, req_grad=True, keep_bn=False):\n",
    "    '''Set requires_grad to `req_grad` for all parameters in `module_groups`.\n",
    "    If `keep_bn` is True, batchnorm layers are not changed.'''\n",
    "    \n",
    "    for idx, group in enumerate(module_groups):\n",
    "        for module in group:\n",
    "            for p in module.parameters(recurse=False):\n",
    "                if not keep_bn or not isinstance(module, bn_types): p.requires_grad=req_grad\n",
    "\n",
    "def freeze_to(module_groups, group_idx=-1, keep_bn=False):\n",
    "    '''Freeze model groups up to the group with index `group_idx`. If `group_idx` is None, \n",
    "    freezes the entire model. If `keep_bn` is True, batchnorm layers are not changed.'''\n",
    "    \n",
    "    num_groups = len(module_groups)\n",
    "    slice_freeze = slice(0, group_idx)\n",
    "    if group_idx is not None:\n",
    "        slice_unfreeze = slice(group_idx, None)\n",
    "    \n",
    "    groups_requires_grad(module_groups[slice_freeze], False, keep_bn)\n",
    "\n",
    "    if group_idx is not None:\n",
    "        groups_requires_grad(module_groups[slice_unfreeze], True)\n",
    "            \n",
    "def unfreeze(module_groups):\n",
    "    '''Unfreezes the entire model.'''\n",
    "    \n",
    "    groups_requires_grad(module_groups, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorchlb.unet import UNet    # Fix this!\n",
    "\n",
    "unet = UNet(3, 2)\n",
    "hooks_act = Hooks(unet, [unet.l1_, unet.l2_, unet.l3_, unet.l4_], calculate_stats(True))\n",
    "hooks_wei = Hooks(unet, [unet.l1_.dconv[0], unet.l2_.dconv[0], unet.l3_.dconv[0], unet.l4_.dconv[0]], \n",
    "                  calculate_stats(False, True))\n",
    "\n",
    "xb = torch.randn([2, 3, 128, 128])\n",
    "pred = unet(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l1_.dconv.0': {'weights': {'mean': [0.0029068964067846537],\n",
       "   'std': [0.2779395282268524]}},\n",
       " 'l2_.dconv.0': {'weights': {'mean': [0.0003165746165905148],\n",
       "   'std': [0.0591021291911602]}},\n",
       " 'l3_.dconv.0': {'weights': {'mean': [-0.00012993672862648964],\n",
       "   'std': [0.041721682995557785]}},\n",
       " 'l4_.dconv.0': {'weights': {'mean': [3.7154190067667514e-05],\n",
       "   'std': [0.02945738658308983]}}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hooks_wei.storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=list(unet.l1_.dconv.children())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReLU(inplace=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(unet.named_modules())[1][1] == unet.l1_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'UNet' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-eceda6f318da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0munet\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'UNet' object is not iterable"
     ]
    }
   ],
   "source": [
    "for m in unet:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
