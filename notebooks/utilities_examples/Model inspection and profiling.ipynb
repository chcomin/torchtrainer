{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples of using the model inspection class and the profiling functions\n",
    "\n",
    "The inspection class can gather model parameters, gradients, activations and activation gradients.\n",
    "\n",
    "The profiling module provide information about cpu and GPU timing, memory usage and flops operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import _torchtrainer as torchtrainer\n",
    "from torchtrainer.models import UNetCustom\n",
    "from torchtrainer.util.inspector import Inspector\n",
    "from torchtrainer.util.profiling import benchmark_model\n",
    "\n",
    "# Dummy data. In a real application this should be a batch of the dataset.\n",
    "batch = torch.rand(8, 1, 224, 224)\n",
    "labels = torch.randint(0, 1000, (8,))\n",
    "\n",
    "model = UNetCustom((3, 3, 3), (1, 1, 1), (16, 32, 64))\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('unetcustom.stage_input.0', 'unetcustom.stage_input.1', 'unetcustom.encoder.stage_0.0.conv1', 'unetcustom.encoder.stage_0.0.bn1', 'unetcustom.encoder.stage_0.0.conv2', 'unetcustom.encoder.stage_0.0.bn2', 'unetcustom.encoder.stage_0.0.residual_adj.0', 'unetcustom.encoder.stage_0.0.residual_adj.1', 'unetcustom.encoder.stage_0.0', 'unetcustom.encoder.stage_0.1.conv1')\n",
      "torch.Size([8, 16, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# Check intermediate activations of the model, as well as gradients and parameters\n",
    "insp = Inspector(model)\n",
    "# For activations, we need to explicitly start tracking to set up the forward hooks.\n",
    "insp.start_tracking_activations()\n",
    "\n",
    "# Apply model to batch\n",
    "res = model(batch)\n",
    "# Dummy calculation of loss and gradients, just as an example.\n",
    "loss = res.sum().backward()\n",
    "\n",
    "# Remove the forward hooks\n",
    "insp.stop_tracking_activations()\n",
    "\n",
    "# Activations\n",
    "acts = insp.get_activations()\n",
    "# Parameters\n",
    "params = insp.get_params()\n",
    "# Gradients\n",
    "grads = insp.get_grads()\n",
    "\n",
    "names, values = zip(*acts)\n",
    "print(names[:10])    # Names of the first 10 layers\n",
    "print(values[0].shape)  # Size of the activations of the first layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tracking activations involves copying all the data from the GPU to the CPU (to preserve GPU memory), which is expensive. We can provide and aggregation function that will be applied to the data before copying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('unetcustom.stage_input.0', tensor([-0.9230,  0.6710])), ('unetcustom.stage_input.1', tensor([-0.9230,  0.6709])), ('unetcustom.encoder.stage_0.0.conv1', tensor([-0.6979,  0.4909])), ('unetcustom.encoder.stage_0.0.bn1', tensor([-0.6979,  0.4909])), ('unetcustom.encoder.stage_0.0.conv2', tensor([-0.3816,  0.3757])), ('unetcustom.encoder.stage_0.0.bn2', tensor([-0.3816,  0.3757])), ('unetcustom.encoder.stage_0.0.residual_adj.0', tensor([-0.5522,  0.7290])), ('unetcustom.encoder.stage_0.0.residual_adj.1', tensor([-0.5522,  0.7290])), ('unetcustom.encoder.stage_0.0', tensor([0.0000, 0.8208])), ('unetcustom.encoder.stage_0.1.conv1', tensor([-0.7703,  0.8765])), ('unetcustom.encoder.stage_0.1.bn1', tensor([-0.7703,  0.8765])), ('unetcustom.encoder.stage_0.1.conv2', tensor([-0.7112,  0.6590])), ('unetcustom.encoder.stage_0.1.bn2', tensor([-0.7112,  0.6590])), ('unetcustom.encoder.stage_0.1', tensor([0.0000, 1.1256])), ('unetcustom.encoder.stage_0.2.conv1', tensor([-0.9547,  0.8945])), ('unetcustom.encoder.stage_0.2.bn1', tensor([-0.9547,  0.8945])), ('unetcustom.encoder.stage_0.2.conv2', tensor([-0.7827,  0.7061])), ('unetcustom.encoder.stage_0.2.bn2', tensor([-0.7827,  0.7061])), ('unetcustom.encoder.stage_0.2', tensor([0.0000, 1.4564])), ('unetcustom.encoder.stage_1.0.conv1', tensor([-1.1217,  1.0916])), ('unetcustom.encoder.stage_1.0.bn1', tensor([-1.1217,  1.0916])), ('unetcustom.encoder.stage_1.0.conv2', tensor([-0.7673,  0.7793])), ('unetcustom.encoder.stage_1.0.bn2', tensor([-0.7673,  0.7793])), ('unetcustom.encoder.stage_1.0.residual_adj.0', tensor([-1.1618,  1.5366])), ('unetcustom.encoder.stage_1.0.residual_adj.1', tensor([-1.1618,  1.5366])), ('unetcustom.encoder.stage_1.0', tensor([0.0000, 1.2659])), ('unetcustom.encoder.stage_1.1.conv1', tensor([-1.1594,  1.1940])), ('unetcustom.encoder.stage_1.1.bn1', tensor([-1.1594,  1.1940])), ('unetcustom.encoder.stage_1.1.conv2', tensor([-1.4665,  1.3792])), ('unetcustom.encoder.stage_1.1.bn2', tensor([-1.4665,  1.3792])), ('unetcustom.encoder.stage_1.1', tensor([0.0000, 1.6857])), ('unetcustom.encoder.stage_1.2.conv1', tensor([-1.7482,  1.7382])), ('unetcustom.encoder.stage_1.2.bn1', tensor([-1.7482,  1.7382])), ('unetcustom.encoder.stage_1.2.conv2', tensor([-1.8150,  1.7123])), ('unetcustom.encoder.stage_1.2.bn2', tensor([-1.8150,  1.7123])), ('unetcustom.encoder.stage_1.2', tensor([0.0000, 2.1260])), ('unetcustom.encoder.stage_2.0.conv1', tensor([-1.7588,  1.8146])), ('unetcustom.encoder.stage_2.0.bn1', tensor([-1.7588,  1.8146])), ('unetcustom.encoder.stage_2.0.conv2', tensor([-2.2179,  1.8321])), ('unetcustom.encoder.stage_2.0.bn2', tensor([-2.2179,  1.8320])), ('unetcustom.encoder.stage_2.0.residual_adj.0', tensor([-1.8377,  2.0530])), ('unetcustom.encoder.stage_2.0.residual_adj.1', tensor([-1.8377,  2.0530])), ('unetcustom.encoder.stage_2.0', tensor([0.0000, 2.8001])), ('unetcustom.encoder.stage_2.1.conv1', tensor([-2.4675,  2.1231])), ('unetcustom.encoder.stage_2.1.bn1', tensor([-2.4675,  2.1231])), ('unetcustom.encoder.stage_2.1.conv2', tensor([-2.0330,  1.7396])), ('unetcustom.encoder.stage_2.1.bn2', tensor([-2.0330,  1.7396])), ('unetcustom.encoder.stage_2.1', tensor([0.0000, 3.8522])), ('unetcustom.encoder.stage_2.2.conv1', tensor([-3.3175,  4.2068])), ('unetcustom.encoder.stage_2.2.bn1', tensor([-3.3174,  4.2067])), ('unetcustom.encoder.stage_2.2.conv2', tensor([-4.8231,  4.4097])), ('unetcustom.encoder.stage_2.2.bn2', tensor([-4.8231,  4.4096])), ('unetcustom.encoder.stage_2.2', tensor([0.0000, 5.5337])), ('unetcustom.decoder.stage_2.0.channel_adj.0', tensor([-8.1802,  6.3834])), ('unetcustom.decoder.stage_2.0.channel_adj.1', tensor([-8.1801,  6.3834])), ('unetcustom.decoder.stage_2.0.interpolate', tensor([0.0000, 6.3834])), ('unetcustom.decoder.stage_2.0', tensor([0.0000, 6.3834])), ('unetcustom.decoder.stage_2.1', tensor([0.0000, 6.3834])), ('unetcustom.decoder.stage_2.2.0.conv1', tensor([-8.2555,  6.7295])), ('unetcustom.decoder.stage_2.2.0.bn1', tensor([-8.2555,  6.7295])), ('unetcustom.decoder.stage_2.2.0.conv2', tensor([-7.2448,  7.0732])), ('unetcustom.decoder.stage_2.2.0.bn2', tensor([-7.2447,  7.0732])), ('unetcustom.decoder.stage_2.2.0.residual_adj.0', tensor([-7.2345,  8.4083])), ('unetcustom.decoder.stage_2.2.0.residual_adj.1', tensor([-7.2345,  8.4083])), ('unetcustom.decoder.stage_2.2.0', tensor([ 0.0000, 10.1810])), ('unetcustom.decoder.stage_1.0.channel_adj.0', tensor([-16.4026,  12.3899])), ('unetcustom.decoder.stage_1.0.channel_adj.1', tensor([-16.4026,  12.3899])), ('unetcustom.decoder.stage_1.0.interpolate', tensor([ 0.0000, 12.3899])), ('unetcustom.decoder.stage_1.0', tensor([ 0.0000, 12.3899])), ('unetcustom.decoder.stage_1.1', tensor([ 0.0000, 12.3899])), ('unetcustom.decoder.stage_1.2.0.conv1', tensor([-10.2688,   9.8198])), ('unetcustom.decoder.stage_1.2.0.bn1', tensor([-10.2687,   9.8197])), ('unetcustom.decoder.stage_1.2.0.conv2', tensor([-9.0991,  8.9121])), ('unetcustom.decoder.stage_1.2.0.bn2', tensor([-9.0991,  8.9121])), ('unetcustom.decoder.stage_1.2.0.residual_adj.0', tensor([-13.9182,  17.9845])), ('unetcustom.decoder.stage_1.2.0.residual_adj.1', tensor([-13.9181,  17.9844])), ('unetcustom.decoder.stage_1.2.0', tensor([ 0.0000, 16.2720])), ('unetcustom.decoder.stage_0.0.interpolate', tensor([ 0.0000, 16.2720])), ('unetcustom.decoder.stage_0.0', tensor([ 0.0000, 16.2720])), ('unetcustom.decoder.stage_0.1', tensor([ 0.0000, 16.2720])), ('unetcustom.decoder.stage_0.2.0.conv1', tensor([-17.1309,  14.7112])), ('unetcustom.decoder.stage_0.2.0.bn1', tensor([-17.1308,  14.7111])), ('unetcustom.decoder.stage_0.2.0.conv2', tensor([-12.9413,  17.4367])), ('unetcustom.decoder.stage_0.2.0.bn2', tensor([-12.9412,  17.4366])), ('unetcustom.decoder.stage_0.2.0.residual_adj.0', tensor([-15.1978,  19.0878])), ('unetcustom.decoder.stage_0.2.0.residual_adj.1', tensor([-15.1977,  19.0877])), ('unetcustom.decoder.stage_0.2.0', tensor([ 0.0000, 24.0649])), ('unetcustom.conv_output', tensor([-64.9684,   9.8039])), ('unetcustom', tensor([-64.9684,   9.8039]))]\n"
     ]
    }
   ],
   "source": [
    "def agg_func(data, module_name, data_type):\n",
    "    return torch.tensor([data.min(), data.max()])\n",
    "\n",
    "insp = Inspector(model, agg_func=agg_func)\n",
    "insp.start_tracking_activations()\n",
    "res = model(batch)\n",
    "insp.stop_tracking_activations()\n",
    "\n",
    "acts = insp.get_activations()\n",
    "print(acts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also only track individual modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('unetcustom.stage_input', tensor([0.0000, 0.6709])), ('unetcustom.encoder.stage_2.0', tensor([0.0000, 2.8001]))]\n"
     ]
    }
   ],
   "source": [
    "insp = Inspector(model, [model.stage_input,model.encoder.stage_2[0]], agg_func)\n",
    "insp.start_tracking_activations()\n",
    "res = model(batch)\n",
    "insp.stop_tracking_activations()\n",
    "\n",
    "acts = insp.get_activations()\n",
    "print(acts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to track activation gradients, but it is important to note that they do not work for a layer if the previous layer has an inplace operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "insp = Inspector(model, [model.stage_input])\n",
    "insp.start_tracking_act_grads()\n",
    "res = model(batch)\n",
    "insp.stop_tracking_act_grads()\n",
    "loss = res.sum().backward()\n",
    "\n",
    "act_grads = insp.get_act_grads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'memory': 0.305875301361084, 'time_cpu': 0.004408836364746094, 'time_gpu': 0.014523391723632812, 'info': ['memory: GiB', 'time_cpu: s', 'time_gpu: s']}\n",
      "{'memory': 0.1378645896911621, 'time_cpu': 0.001367807388305664, 'time_gpu': 0.0030093119144439695, 'info': ['memory: GiB', 'time_cpu: s', 'time_gpu: s']}\n"
     ]
    }
   ],
   "source": [
    "tensor_shape = (8, 1, 224, 224)\n",
    "# Benchmark the model for training\n",
    "stats_train = benchmark_model(model, tensor_shape, no_grad=False, call_backward=True, use_float16=True)\n",
    "# Benchmark for inference\n",
    "stats_val = benchmark_model(model, tensor_shape, no_grad=True, call_backward=False, use_float16=True)\n",
    "# The units for each metric are also included in the dictionary\n",
    "print(stats_train)\n",
    "print(stats_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
